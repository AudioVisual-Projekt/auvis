{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff414c66",
   "metadata": {},
   "source": [
    "# Task 5D – Teil 3: Feature Engineering & Embedding-Generierung\n",
    "\n",
    "**Umfang dieses Notebooks (Teil 3):**\n",
    "\n",
    "1. Laden der vorbereiteten Äußerungsdaten aus Teil 2 (`train_utterances.parquet` und `dev_utterances.parquet`)\n",
    "2. Initialisierung eines geeigneten Satz-Embedding-Modells (`sentence-transformers`)\n",
    "3. Berechnung von numerischen Embeddings für jede Äußerung im Datensatz\n",
    "4. Speicherung der Ergebnisse als `.npz`-Dateien (NumPy-Container) zur späteren Nutzung\n",
    "\n",
    "> **Hinweis:** Dieses Notebook erzeugt die semantischen Repräsentationen der Transkripte und bildet damit die Grundlage für das Clustering in Teil 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acb359",
   "metadata": {},
   "source": [
    "## 1) Bibliotheken laden & Konfiguration vorbereiten\n",
    "\n",
    "In diesem Abschnitt werden alle benötigten Bibliotheken eingebunden und zentrale Konfigurationsparameter aus der Datei `run_config.json` geladen.\n",
    "\n",
    "Dazu zählen insbesondere:\n",
    "\n",
    "* das Projektverzeichnis (`PROJECT_ROOT`)\n",
    "* die Speicherorte der Embedding-Dateien (`train_embeddings.npz`, `dev_embeddings.npz`)\n",
    "* vorbereitende Einstellungen für das Clustering\n",
    "\n",
    "> Ziel ist es, eine konsistente Arbeitsumgebung zu schaffen, auf der alle folgenden Schritte aufbauen können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed261af-9086-48dc-978c-ad032bd670bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b917513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 1) Bibliotheken laden & Konfiguration vorbereiten\n",
    "# ---------------------------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Projektpfad und zentrale Parameter aus run_config.json laden\n",
    "# - Pfade zu vorbereiteten Parquet-Dateien definieren (aus Teil 2)\n",
    "\n",
    "from pathlib import Path  # Für systemunabhängige Pfadangaben\n",
    "import pandas as pd       # Für Datenmanipulation mit DataFrames\n",
    "import json               # Für das Einlesen der Konfigurationsdatei\n",
    "\n",
    "# Projektverzeichnis festlegen (wurde in Teil 1 angelegt)\n",
    "PROJECT_ROOT = Path.home() / \"AUVIS/task5D_ml_prototype\"\n",
    "\n",
    "# Pfad zur Konfigurationsdatei\n",
    "cfg_path = PROJECT_ROOT / \"run_config.json\"\n",
    "\n",
    "# Konfigurationsdatei laden (JSON)\n",
    "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# Pfade zu den vorbereiteten Parquet-Dateien (aus Teil 2)\n",
    "PREPARED_DIR   = PROJECT_ROOT / \"data\" / \"prepared\"\n",
    "PARQUET_TRAIN  = PREPARED_DIR / \"train_utterances.parquet\"\n",
    "PARQUET_DEV    = PREPARED_DIR / \"dev_utterances.parquet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5b35a",
   "metadata": {},
   "source": [
    "## 2) Modell vorbereiten\n",
    "\n",
    "In diesem Schritt wird ein vortrainiertes **englisches Sprachmodell** aus der Bibliothek `sentence-transformers` geladen. Dieses Modell wandelt die transkribierten Äußerungen in numerische Repräsentationen (sogenannte *Embeddings*) um.\n",
    "\n",
    "Diese Embeddings dienen als Grundlage für weiterführende Verfahren wie **Clustering**, **Klassifikation** oder **Ähnlichkeitsanalysen**. Das Modell basiert auf einem Transformer-Encoder und ist dafür optimiert, semantisch ähnliche Texte in ähnliche Vektoren zu überführen.\n",
    "\n",
    "> Hinweis: Für die CHiME-Task werden überwiegend englischsprachige Daten verarbeitet. Entsprechend wird ein **englisch fine-getuntes Base-Modell** (z. B. `all-mpnet-base-v2`) verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8e3ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen: sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 2) Modell vorbereiten\n",
    "# ---------------------------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Ein vortrainiertes Sentence-Transformer-Modell laden\n",
    "# - Das Modell dient zur Erzeugung von Satz-Embeddings (numerische Repräsentationen)\n",
    "# - Diese Embeddings sind später Grundlage für Clustering/Klassifikation\n",
    "\n",
    "from sentence_transformers import SentenceTransformer  # HF-kompatible Bibliothek für Satz-Embeddings\n",
    "\n",
    "# Modellwahl:\n",
    "# - \"all-mpnet-base-v2\" ist ein leistungsstarkes englisches Modell\n",
    "# - Gute semantische Trennung auch bei kurzen Texten\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Modell laden (aus Hugging Face Hub)\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Bestätigung\n",
    "print(\"Modell erfolgreich geladen:\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23af777",
   "metadata": {},
   "source": [
    "## 3) Embeddings berechnen\n",
    "\n",
    "In diesem Schritt werden die transkribierten Äußerungen aus dem vorbereiteten Parquet-Format geladen und mit dem geladenen `SentenceTransformer`-Modell in numerische Vektoren (Embeddings) umgewandelt.\n",
    "\n",
    "Jede Äußerung erhält ein semantisches **Satz-Embedding**, das ihren Inhalt in einem mehrdimensionalen Vektorraum repräsentiert. Diese Embeddings sind die Grundlage für alle nachfolgenden Verfahren wie Clustering oder Klassifikation.\n",
    "\n",
    "**Ziel:**\n",
    "\n",
    "* Einlesen der vorbereiteten Daten (Train / Dev)\n",
    "* Batchweise Berechnung der Embeddings mit `model.encode(...)`\n",
    "* Rückgabe als DataFrame zur Weiterverarbeitung oder Persistierung\n",
    "\n",
    "> Hinweis: Für größere Datenmengen kann optional GPU-Beschleunigung oder Batch-Size-Tuning verwendet werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19fedda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Daten: (19076, 7) Sessions: 56\n",
      "Dev-Daten:   (8561, 7) Sessions: 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50d45397ba74253b291408e1ebaa03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756b6b28b16e495da87d59bf7f113d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 3) Embeddings berechnen\n",
    "# ---------------------------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Eingelesene Äußerungen aus den vorbereiteten Parquet-Dateien laden\n",
    "# - Textspalte (Standard: \"text\") in semantische Embeddings umwandeln\n",
    "# - Nutzung des geladenen SentenceTransformer-Modells (GPU optional)\n",
    "# - Rückgabe als NumPy-Array (ein Vektor pro Äußerung)\n",
    "\n",
    "def embed_dataframe(df, model, text_column=\"text\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Berechnet Satz-Embeddings für eine DataFrame-Spalte mit Text.\n",
    "\n",
    "    Parameter:\n",
    "    - df: Pandas DataFrame mit Textspalte\n",
    "    - model: SentenceTransformer-Modell\n",
    "    - text_column: Name der Textspalte im DataFrame (Standard: \"text\")\n",
    "    - batch_size: Anzahl der Texte pro Mini-Batch (je nach GPU-Speicher anpassbar)\n",
    "\n",
    "    Rückgabe:\n",
    "    - NumPy-Array mit einem Vektor pro Zeile im DataFrame\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas()  # Fortschrittsbalken aktivieren\n",
    "\n",
    "    # Modellencode: Liste von Strings → Liste von Embeddings\n",
    "    embeddings = model.encode(\n",
    "        df[text_column].tolist(),\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "# ----------------------------\n",
    "# Daten laden & Embedding berechnen\n",
    "# ----------------------------\n",
    "\n",
    "# Immer die neuen Multi-Session-Dateien verwenden\n",
    "PARQUET_TRAIN = PROJECT_ROOT / \"data\" / \"prepared\" / \"train_utterances_multisession.parquet\"\n",
    "PARQUET_DEV   = PROJECT_ROOT / \"data\" / \"prepared\" / \"dev_utterances_multisession.parquet\"\n",
    "\n",
    "df_train = pd.read_parquet(PARQUET_TRAIN)\n",
    "df_dev   = pd.read_parquet(PARQUET_DEV)\n",
    "\n",
    "print(\"Train-Daten:\", df_train.shape, \"Sessions:\", df_train['session_id'].nunique())\n",
    "print(\"Dev-Daten:  \", df_dev.shape,   \"Sessions:\", df_dev['session_id'].nunique())\n",
    "\n",
    "# Embeddings erzeugen\n",
    "emb_train = embed_dataframe(df_train, model)\n",
    "emb_dev   = embed_dataframe(df_dev, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d577e",
   "metadata": {},
   "source": [
    "## 4) Embeddings speichern\n",
    "\n",
    "Nach der erfolgreichen Berechnung der Satz-Embeddings für Trainings- und Entwicklungsdaten speichern wir die Ergebnisse für die spätere Weiterverarbeitung.\n",
    "\n",
    "**Ziel:** Persistenz der Embeddings als `.npy`-Dateien (NumPy-Format) zur effizienten Wiederverwendung in Downstream-Schritten wie Clustering oder Klassifikation.\n",
    "Optional könnten die Embeddings auch als `.parquet` oder `.hdf5` gespeichert werden – das `.npy`-Format bietet jedoch eine besonders einfache und performante Handhabung bei numerischen Arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb091ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert:\n",
      " - train_embeddings.npz\n",
      " - dev_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 4) Embeddings speichern\n",
    "# ---------------------------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Die zuvor berechneten Embeddings als komprimierte NumPy-Dateien sichern\n",
    "# - Format: .npz (komprimiert, speichert mehrere Arrays in einer Datei)\n",
    "# - Je Split (Train/Dev) werden:\n",
    "#   - die eindeutigen Äußerungs-IDs (\"utt_id\") und\n",
    "#   - die zugehörigen Embedding-Vektoren gespeichert\n",
    "\n",
    "import numpy as np  # NumPy für effiziente numerische Arrays\n",
    "\n",
    "# Zielverzeichnis für Embeddings definieren\n",
    "out_dir = PROJECT_ROOT / \"data\" / \"embeddings\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)  # Verzeichnis anlegen (falls nicht vorhanden)\n",
    "\n",
    "# Speicherung: Train-Split\n",
    "np.savez_compressed(\n",
    "    out_dir / \"train_embeddings.npz\",        # Zielpfad\n",
    "    utt_ids=df_train[\"utt_id\"].astype(str).values,  # Äußerungs-IDs als Strings\n",
    "    embeddings=emb_train                     # Embedding-Vektoren (z. B. 768-dim)\n",
    ")\n",
    "\n",
    "# Speicherung: Dev-Split\n",
    "np.savez_compressed(\n",
    "    out_dir / \"dev_embeddings.npz\",          # Zielpfad\n",
    "    utt_ids=df_dev[\"utt_id\"].astype(str).values,    # Äußerungs-IDs als Strings\n",
    "    embeddings=emb_dev                       # Embedding-Vektoren\n",
    ")\n",
    "\n",
    "# Bestätigung der gespeicherten Dateien\n",
    "print(\"Gespeichert:\")\n",
    "for f in out_dir.glob(\"*.npz\"):\n",
    "    print(\" -\", f.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6f1e7",
   "metadata": {},
   "source": [
    "### ✅ Nächste Schritte (für Teil 4)\n",
    "\n",
    "Fahre mit dem nächsten Notebook **Teil 4: Clustering der Äußerungen zu Gesprächseinheiten** fort, sobald die Embeddings erfolgreich gespeichert wurden:\n",
    "\n",
    "* Extraktion semantischer Merkmale mittels vortrainierter Sentence-Embedding-Modelle (`sentence-transformers`)\n",
    "* Speicherung der Äußerungs-Embeddings als `.npz`-Dateien (komprimierte Vektor-Repräsentationen)\n",
    "* Optionale Vorbereitung weiterführender Merkmale (prosodisch, visuell, kontextuell)\n",
    "* Ziel: Grundlage schaffen für sprecherübergreifendes Clustering in Teil 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824a67c-5c1c-4a4c-8fb6-cfcca3b059f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
