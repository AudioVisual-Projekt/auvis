{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d319d9aa-cd7d-4c8a-b489-7f085d7025b8",
   "metadata": {},
   "source": [
    "# CHiME‑9 Task 1 – Subtask 5: Videoanalyse‑Pipeline\n",
    "\n",
    "Dieses Notebook dokumentiert und demonstriert die Video‑ und Blickanalyse‑Pipeline für Subtask 5 der CHiME‑9 Task 1 (MCoRec).  \n",
    "Der Fokus liegt auf der Ableitung von Sprecher‑Clusterings aus Video‑basierten Signalen (Seating, ASD‑Tracks, Kopf‑Yaw/Gaze) und auf der Auswertung der resultierenden Cluster im Kontext der Challenge‑Daten. Das Notebook basiert auf diesem Branch: https://github.com/AudioVisual-Projekt/auvis/tree/Feature/task_5_alternative_cluster\n",
    "\n",
    "Die Notebook‑Demo zeigt:\n",
    "\n",
    "- wie die zentrale Inferenz (`main.py`) für Gesprächs‑Clustering aufgerufen wird,\n",
    "- wie nachgelagerte Gaze‑ und Distanzberechnungen mit `post_step.py` funktionieren,\n",
    "- wie Video‑Cluster mit `eval_video_cluster_quality.py` bewertet werden,\n",
    "- wie ASD‑Overlap‑ und Seating‑Baselines evaluiert werden,\n",
    "- wie die resultierenden JSON‑ und NPY‑Outputs weiter analysiert und visualisiert wurden(z. B. Distanz‑Histogramme und Correct‑Rate‑Kurven).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81723c9-ed57-48c3-b9eb-d9a240b45314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations- und Umgebungs-Hinweise\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "#\n",
    "# Projektstruktur (relevante Ausschnitte): \n",
    "# auvis/\n",
    "#   team_c/\n",
    "#     main.py\n",
    "#     post_step.py\n",
    "#     plots_py.py\n",
    "#     eval_asd_overlap_baseline.py\n",
    "#     eval_seating.py\n",
    "#     eval_video_cluster_quality.py\n",
    "#     eval_sessions.py\n",
    "#     data-bin/\n",
    "#       dev/\n",
    "#         sessionXXX/\n",
    "#       output/\n",
    "#       output_gaze/\n",
    "#       output_gaze_plots_spectral/\n",
    "#         speaker_clustering_eval.json\n",
    "#         speaker_distance_matrices.json\n",
    "#         gaze_distance_gt_bins/...\n",
    "\n",
    "# === Pfadkonfiguration für das Notebook ===\n",
    "\n",
    "# Passe diesen Pfad an dein lokales Repo an (Ordner, der 'team_c' enthält).\n",
    "PROJECT_ROOT = Path(\"/path/to/auvis/team_c\").resolve()\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Daten-Wurzel (analog zu den Skripten, die mit 'data-bin' arbeiten).\n",
    "DATABIN_ROOT = PROJECT_ROOT / \"data-bin\"\n",
    "\n",
    "DEV_SESSIONS_ROOT = DATABIN_ROOT / \"dev\"\n",
    "OUTPUT_ROOT = DATABIN_ROOT / \"output\"\n",
    "OUTPUT_GAZE_ROOT = DATABIN_ROOT / \"output_gaze\"\n",
    "OUTPUT_GAZE_PLOTS_SPECTRAL = DATABIN_ROOT / \"output_gaze_plots_spectral\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATABIN_ROOT:\", DATABIN_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c9d9a-852b-4d45-9961-7a59b32d8f4d",
   "metadata": {},
   "source": [
    "## Projektüberblick & Main-Skript (`main.py`)\n",
    "\n",
    "Das Skript `main.py` ist der zentrale Entry‑Point für das zeit‑ und semantikbasierte Gesprächs‑Clustering in Subtask 5.[file:8]  \n",
    "Es definiert eine Klasse `InferenceEngine`, die für jede MCoRec‑Session Metadaten lädt, Sprechersegmente aus ASD‑Tracks ableitet und mehrere Clustering‑Varianten erzeugt.\n",
    "\n",
    "Die wichtigsten Verarbeitungsschritte innerhalb von `InferenceEngine.mcorec_session_infer(...)` sind:\n",
    "1. Laden der Session‑Metadaten (`metadata.json`) und Sammeln aller ASD‑Tracks pro Sprecher.\n",
    "2. Ableitung von Sprecher‑Aktivitätssegmenten via `get_speaker_activity_segments` aus `src.cluster.convspks` (Zeitsegmente im UEM‑Fenster).\n",
    "3. Berechnung von Gesprächs‑Scores pro Sprecherpaar mit `calculate_conversation_scores` und zeitbasiertes Agglomerativ‑Clustering (`clusterspeakers`), das nach `speakertocluster_time.json` geschrieben wird.\n",
    "4. Rein semantisches Clustering mit `semantic_cluster_speakers`, das semantische Embeddings aus `team_c/data-bin` nutzt und `speakertocluster_semantic.json` erzeugt. \n",
    "5. Hybrid‑Clustering, das Zeit‑ und Semantik‑Informationen kombiniert (`hybrid_cluster_speakers`) und `speakertocluster_hybrid.json` schreibt.\n",
    "\n",
    "Die `main(...)`‑Funktion parst ein Session‑Glob (`--sessiondir`), iteriert über alle passenden Session‑Ordner, legt pro Session ein Output‑Verzeichnis unter `team_c/data-bin/<outputdirname>/<session>` an und ruft für jede Session `InferenceEngine.mcorec_session_infer(...)` auf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97574f45-cc91-4299-b998-5b5dc8d4b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimaler Wrapper um die Main-Pipeline (Zeit/Semantik/Hybrid) \n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from main import InferenceEngine  # Modul liegt im PROJECT_ROOT (team_c). \n",
    "\n",
    "\n",
    "def run_time_semantic_hybrid_clustering(\n",
    "    session_glob: str,\n",
    "    output_dirname: str = \"output_semantic\",\n",
    "    max_length: int = 15,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run the unified time / semantic / hybrid clustering pipeline for all\n",
    "    sessions matching a given glob pattern (relative to PROJECT_ROOT).\n",
    "\n",
    "    This is a thin wrapper around main.py that mirrors its behavior without\n",
    "    changing any core logic.\n",
    "    \"\"\"\n",
    "    session_pattern = os.path.join(str(PROJECT_ROOT), session_glob)\n",
    "    session_dirs = [p for p in glob(session_pattern) if os.path.isdir(p)]\n",
    "    session_dirs = sorted(session_dirs)\n",
    "\n",
    "    if not session_dirs:\n",
    "        print(\"No sessions found for pattern:\", session_pattern)\n",
    "        return\n",
    "\n",
    "    engine = InferenceEngine(max_length=max_length)  # same default as in main.py. \n",
    "\n",
    "    output_base = DATABIN_ROOT / output_dirname\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Running inference for {len(session_dirs)} sessions...\")\n",
    "    for session_dir in session_dirs:\n",
    "        session_name = os.path.basename(os.path.normpath(session_dir))\n",
    "        output_dir = output_base / session_name\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\nSession: {session_name}\")\n",
    "        print(\"  session_dir:\", session_dir)\n",
    "        print(\"  output_dir :\", output_dir)\n",
    "\n",
    "        # Delegate all heavy lifting to the original InferenceEngine. \n",
    "        engine.mcorec_session_infer(session_dir=session_dir, output_dir=str(output_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f21c65-930a-4299-a1b2-e05c237e6de5",
   "metadata": {},
   "source": [
    "## Utility-Module: I/O, Pfade und Basis-Helfer (`post_step.py`)\n",
    "\n",
    "Das Skript `post_step.py` implementiert einen eigenständigen Post‑Processing‑Schritt, der auf bereits erzeugten Session‑Outputs aufsetzt (Clusterings, Sitzgeometrie, Gaze‑Tracks).\n",
    "Es enthält zunächst generische Helfer für JSON‑I/O (`read_json`, `write_json`), Pfad‑Discovery (`discover_teamc_root`) und Normalisierung von Sprecher‑IDs (`norm_spk_id`, `intersection_keys`).\n",
    "\n",
    "Diese Utilities werden in den späteren Funktionen zur Distanzberechnung, zu agglomerativem Clustering und zu spektraler Gaze‑Analyse wiederverwendet.\n",
    "Für das Notebook ist es daher sinnvoll, die zentralen Helfer in einer kompakten Form zu importieren und bei Bedarf zu nutzen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b56e7-eb04-4ac4-802d-8f5e717d5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import post_step as pst   \n",
    "\n",
    "\n",
    "def read_json(path: Path):\n",
    "    \"\"\"Thin wrapper around post_step.read_json for convenient use in the notebook.\"\"\"\n",
    "    return pst.read_json(path)  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "def write_json(path: Path, obj) -> None:\n",
    "    \"\"\"Thin wrapper around post_step.write_json for convenient use in the notebook.\"\"\"\n",
    "    return pst.write_json(path, obj)  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "def discover_teamc_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Discover the 'team_c' repo root by walking up from 'start',\n",
    "    mirroring post_step.discover_teamc_root. [file:3]\n",
    "    \"\"\"\n",
    "    return pst.discover_teamc_root(start)  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "def norm_spk_id(x) -> str:\n",
    "    \"\"\"\n",
    "    Normalize speaker identifiers to 'spk<int>' IDs, reusing the original helper. [file:3]\n",
    "    \"\"\"\n",
    "    return pst.norm_spk_id(x)  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "def intersection_keys(a: Dict[str, int], b: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Return sorted intersection of dict keys (speaker IDs). [file:3]\"\"\"\n",
    "    return pst.intersection_keys(a, b)  # type: ignore[attr-defined]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb01ac9-9f93-427c-875a-dd4aa0e3d175",
   "metadata": {},
   "source": [
    "## Utility-Module: ASD-, Gaze- und Geometrie-Verarbeitung (`post_step.py`)\n",
    "\n",
    "Im nächsten Block von `post_step.py` finden sich Loader und Helfer für Sitzgeometrie, ASD‑Zuordnungen und Gaze‑Statistiken:\n",
    "\n",
    "- `load_seat_geometry(npz_path)`: lädt `seatgeometry.npz` mit normalisierter Sitzdistanzmatrix `dist_seat`, Sitzwinkeln `theta_deg` und `person_ids`. \n",
    "- `load_asd_mapping(asd_json_path)`: lädt `asdseatmatching.json` und erzeugt eine robuste Abbildung von ASD‑Track‑IDs auf Personen‑IDs, inkl. Fallback‑Logik über `assignments_all`.[file:3]  \n",
    "- `load_gaze(gaze_path)`: liest `gazetracks.json` und liefert für jede Person Median‑Yaw, IQR, Stichprobenanzahl und optional die volle Yaw‑Sample‑Serie.\n",
    "- Zirkuläre Hilfsfunktionen `circ_abs_diff_deg` und `circ_signed_diff_deg` behandeln Winkel auf dem Kreis korrekt.\n",
    "\n",
    "Diese Funktionen kapseln die Rohdaten‑Formate der CHiME‑9‑Video‑Annotationsdateien und sind die Basis für die spätere Distanz‑ und Similaritätsberechnung zwischen Sprechern.[file:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d5ebc-fa8b-4113-b1ef-96f3fe363f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutzung der Geometrie- und Gaze-Utilities in einem Beispiel-Wrapper\n",
    "def load_session_geometry_and_gaze(session_dir: Path):\n",
    "    \"\"\"\n",
    "    Load seat geometry, ASD mapping and gaze tracks for a single session.\n",
    "\n",
    "    Expected files in 'session_dir':\n",
    "    - seatgeometry.npz\n",
    "    - asdseatmatching.json\n",
    "    - gazetracks.json (optional) [file:3]\n",
    "    \"\"\"\n",
    "    seat_npz = session_dir / \"seatgeometry.npz\"\n",
    "    asd_json = session_dir / \"asdseatmatching.json\"\n",
    "    gaze_json = session_dir / \"gazetracks.json\"\n",
    "\n",
    "    if not seat_npz.exists() or not asd_json.exists():\n",
    "        raise FileNotFoundError(\"Missing seatgeometry.npz or asdseatmatching.json in \"\n",
    "                                f\"{session_dir}\")\n",
    "\n",
    "    geom = pst.load_seat_geometry(seat_npz)  # type: ignore[attr-defined]\n",
    "    asd_ids, asd_to_person = pst.load_asd_mapping(asd_json)  # type: ignore[attr-defined]\n",
    "\n",
    "    if gaze_json.exists():\n",
    "        gaze_by_person = pst.load_gaze(gaze_json)  # type: ignore[attr-defined]\n",
    "    else:\n",
    "        gaze_by_person = {}\n",
    "\n",
    "    return geom, asd_ids, asd_to_person, gaze_by_person\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dd567-9a66-4abc-b49c-16437ebe57b9",
   "metadata": {},
   "source": [
    "## Utility-Module: Sprecher-Distanzmatrizen & Gaze-Integration (`post_step.py`)\n",
    "\n",
    "Auf Basis von Sitzgeometrie, ASD‑Mapping und Gaze‑Daten baut `build_speaker_distance_matrices(...)` drei M×M‑Matrizen (M = Anzahl ASD‑Tracks):\n",
    "\n",
    "- `D_seat`: reine Sitzdistanz (0..1) entlang der kreisförmigen Sitzordnung.\n",
    "- `D_seat_gaze`: Sitzdistanz, skaliert mit einem kontinuierlichen Gaze‑Interaktionsfaktor, der mutual/one‑way Gaze mit Gewichten `WMUTUAL` und `WONEWAY` einbezieht.\n",
    "- `D_gaze_only`: reine Interaktionsdistanz (Basis 1.0), reduziert durch Gaze‑Kontakt; wird 0 bei starker gegenseitiger Blickinteraktion.\n",
    "\n",
    "Die Funktion erzeugt zusätzlich ein `meta`‑Dictionary mit Sprecherreihenfolge, Sitzzuordnung, Gaze‑Nutzungsstatistik und zusammengefassten Metriken wie quantilen von mutual/avg‑look‑Werten.\n",
    "Diese Daten werden später sowohl für agglomeratives Clustering als auch für spektrale Gaze‑Graphen verwendet und global in `speaker_distance_matrices.json` aggregiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d1415-2eb8-4a84-8a26-d1f517a8e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Distanzmatrizen für eine Session berechnen und inspizieren \n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def build_session_distance_matrices(session_dir: Path):\n",
    "    \"\"\"\n",
    "    Compute D_seat, D_seat_gaze and D_gaze_only for a single session\n",
    "    and return them together with the meta information. [file:3]\n",
    "    \"\"\"\n",
    "    geom, asd_ids, asd_to_person, gaze_by_person = load_session_geometry_and_gaze(\n",
    "        session_dir\n",
    "    )\n",
    "\n",
    "    d_seat, d_seat_gaze, d_gaze_only, meta = pst.build_speaker_distance_matrices(\n",
    "        geom=geom,\n",
    "        asdids=asd_ids,\n",
    "        asdtoperson=asd_to_person,\n",
    "        gazebyperson=gaze_by_person,\n",
    "    )  # type: ignore[attr-defined]\n",
    "\n",
    "    print(\"Speaker order:\", meta.get(\"speakerorder\"))\n",
    "    print(\"D_seat shape      :\", d_seat.shape)\n",
    "    print(\"D_seat_gaze shape :\", d_seat_gaze.shape)\n",
    "    print(\"D_gaze_only shape :\", d_gaze_only.shape)\n",
    "\n",
    "    return d_seat, d_seat_gaze, d_gaze_only, meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe2de8-ee69-41f4-8868-fb496319c62e",
   "metadata": {},
   "source": [
    "## Utility-Module: Agglomeratives Clustering & Spektrale Gaze-Graphen (`post_step.py`)\n",
    "\n",
    "Für die Distanzmatrizen implementiert `post_step.py` mehrere Clustering‑Bausteine:\n",
    "\n",
    "- `agglo_labels_from_precomputed_D(D, k)`: Agglomeratives Clustering mit average‑Linkage auf einer vorkomputierten Distanzmatrix (`metric=\"precomputed\"` bzw. älteren `affinity`‑API‑Fallback).[file:3]  \n",
    "- `choose_k_without_gt(D, kmin, kmax)`: Auswahl von k allein aus D durch Maximierung des Silhouette‑Scores; k wird auf \\([2, \\min(k_{\\max}, n)]\\) beschränkt. \n",
    "\n",
    "Für Gaze‑basierte Graphen werden Similaritätsmatrizen `S` berechnet:\n",
    "\n",
    "- `build_mutual_similarity_knn(meta, knn, gamma)`: mutual‑Gaze‑Graph, Similarität \\(\\text{mutual} = \\min(p_{i\\to j}, p_{j\\to i})\\), mit Potenzschärfung `gamma` und optionaler kNN‑Sparsifizierung.  \n",
    "- `build_similarity_avg_knn(meta, knn, gamma)`: Similarität \\(\\text{avglook} = 0.5(p_{i\\to j} + p_{j\\to i})\\) als weniger strikte Alternative.\n",
    "\n",
    "Die spektralen Varianten wählen k über eine gewichtete Modularity‑Metrik auf dem Similaritätsgraphen:\n",
    "\n",
    "- `spectral_cluster_auto_k(S, kmin, kmax)` liefert Clusterlabels und ein Info‑Dict mit `chosen_k`, `scores` und Parametern. \n",
    "- `evaluate_spectral_mutual(...)` und `evaluate_spectral_similarity(...)` wenden diese Strategie auf alle Sessions an, schreiben pro Session `speakertocluster_spectral*.json` und sammeln ARI/F1‑Scores sowie Graph‑Statistiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621c38a-af39-486d-af35-55d3ae4d8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: k-Auswahl und spektrales Clustering für eine Session\n",
    "\n",
    "def spectral_clustering_for_session(session_dir: Path):\n",
    "    \"\"\"\n",
    "    Run mutual- and avg-based spectral clustering for a single session,\n",
    "    mirroring the logic inside post_step.evaluate_spectral_mutual /\n",
    "    evaluate_spectral_similarity, but keeping side-effects minimal. [file:3]\n",
    "    \"\"\"\n",
    "    meta_path = session_dir / \"speaker_distance_meta.json\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(\"speaker_distance_meta.json not found in \"\n",
    "                                f\"{session_dir}. Did you run post_step.build_distance_matrices?\")\n",
    "\n",
    "    meta = read_json(meta_path)\n",
    "    spk_order = meta.get(\"speakerorder\", [])\n",
    "    if not spk_order or len(spk_order) < 2:\n",
    "        print(\"Not enough speakers for spectral clustering.\")\n",
    "        return\n",
    "\n",
    "    n = len(spk_order)\n",
    "    print(\"Number of speakers:\", n)\n",
    "\n",
    "    # Mutual-gaze similarity graph and clustering. \n",
    "    s_mutual = pst.build_mutual_similarity_knn(  # type: ignore[attr-defined]\n",
    "        meta=meta,\n",
    "        knn=pst.SPECTRAL_KNN,       # uses the same constants as in post_step.py\n",
    "        gamma=pst.SPECTRAL_GAMMA,\n",
    "    )\n",
    "    labels_mutual, info_mutual = pst.spectral_cluster_auto_k(  # type: ignore[attr-defined]\n",
    "        S=s_mutual,\n",
    "        kmin=pst.SPECTRAL_KMIN,\n",
    "        kmax=pst.SPECTRAL_KMAX,\n",
    "    )\n",
    "    print(\"Mutual-gaze chosen k:\", info_mutual.get(\"chosenk\"))\n",
    "\n",
    "    # Avg-look similarity graph and clustering. [file:3]\n",
    "    s_avg = pst.build_similarity_avg_knn(  # type: ignore[attr-defined]\n",
    "        meta=meta,\n",
    "        knn=pst.SPECTRAL_KNN,\n",
    "        gamma=pst.SPECTRAL_GAMMA,\n",
    "    )\n",
    "    labels_avg, info_avg = pst.spectral_cluster_auto_k(  # type: ignore[attr-defined]\n",
    "        S=s_avg,\n",
    "        kmin=pst.SPECTRAL_KMIN,\n",
    "        kmax=pst.SPECTRAL_KMAX,\n",
    "    )\n",
    "    print(\"Avg-look chosen k:\", info_avg.get(\"chosenk\"))\n",
    "\n",
    "    return {\n",
    "        \"speakerorder\": spk_order,\n",
    "        \"labels_mutual\": labels_mutual,\n",
    "        \"labels_avg\": labels_avg,\n",
    "        \"info_mutual\": info_mutual,\n",
    "        \"info_avg\": info_avg,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b20eb-74d4-406c-b764-04e0d6754268",
   "metadata": {},
   "source": [
    "## Post-Processing Main-Step (`post_step.main`) und globale Eval-JSONs\n",
    "\n",
    "Die `main()`‑Funktion von `post_step.py` ist als eigenständiger Run‑Step konzipiert, der typischerweise nach der Inferenz mit `main.py` ausgeführt wird.\n",
    "Sie findet automatisch das `team_c`‑Root, setzt `data-bin` als Daten‑ und Output‑Wurzel (`output_gaze` für Gaze‑Outputs, Labels‑Wurzel für GT) und führt dann vier Hauptschritte durch:\n",
    "\n",
    "1. `evaluate_existing_assignments`: Bewertung vorhandener Sitz‑Baselines (`speakertocluster_seat*.json`) gegen Ground‑Truth‑Labels, Ergebnis u. a. in `speaker_clustering_eval.json` mit per‑Session‑ und Durchschnittsmetriken. \n",
    "2. `build_distance_matrices`: Erzeugt pro Session `speaker_distance_seat.npy`, `speaker_distance_seatgaze.npy`, `speaker_distance_gazeonly.npy` plus `speaker_distance_meta.json` sowie einen globalen Dump `speaker_distance_matrices.json`.\n",
    "3. `evaluate_agglo_sweep`: Agglomeratives Clustering für `seat`, `seatgaze` und `gazeonly` mit automatischer k‑Auswahl via Silhouette; schreibt `speakertocluster_agglo*.json` und fasst ARI/F1‑Scores zusammen.\n",
    "4. `evaluate_spectral_mutual` und `evaluate_spectral_similarity`: spektrale Gaze‑Cluster, k‑Auswahl über gewichtete Modularity; schreibt per‑Session‑Predictions und fasst Metriken in `speaker_clustering_eval.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15535f-9d3f-441c-9d3c-ff11888eb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompletten Post-Processing-Step ausführen \n",
    "\n",
    "def run_post_step(\n",
    "    output_root: Optional[Path] = None,\n",
    "    labels_root: Optional[Path] = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run the full post-processing pipeline on existing per-session outputs.\n",
    "\n",
    "    This is a convenience wrapper that delegates to post_step.main(), which\n",
    "    internally:\n",
    "      - evaluates existing seat-based baselines,\n",
    "      - builds distance matrices (seat, seat+gaze, gaze-only),\n",
    "      - runs agglomerative clustering sweeps,\n",
    "      - runs spectral clustering on gaze graphs,\n",
    "      - writes summary JSONs 'speaker_clustering_eval.json' and\n",
    "        'speaker_distance_matrices.json'. [file:3][file:9][file:10]\n",
    "    \"\"\"\n",
    "    # The original main() uses global constants OUTPUT_ROOT and LABELS_ROOT\n",
    "    # if set; here we mimic that behavior by temporarily overriding them. \n",
    "    if output_root is not None:\n",
    "        pst.OUTPUT_ROOT = output_root  # type: ignore[attr-defined]\n",
    "    if labels_root is not None:\n",
    "        pst.LABELS_ROOT = labels_root  # type: ignore[attr-defined]\n",
    "\n",
    "    pst.main()  # type: ignore[attr-defined]\n",
    "\n",
    "    # Optionale Rückgabe: globaler Distance-Dump, falls vorhanden. \n",
    "    global_dump_path = OUTPUT_GAZE_ROOT / \"speaker_distance_matrices.json\"\n",
    "    if global_dump_path.exists():\n",
    "        return read_json(global_dump_path)\n",
    "\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3c8a5-4e94-4f34-ac59-12f7429acade",
   "metadata": {},
   "source": [
    "## Video-Cluster-Qualität, ASD-Overlap-Baseline & Seating-Evaluation\n",
    "\n",
    "Zusätzlich zu `main.py` und `post_step.py` gibt es drei Evaluationsskripte, die speziell für die Analyse von Video‑basierten Clustern relevant sind.\n",
    "\n",
    "1. **ASD-Overlap-Baseline (`eval_asd_overlap_baseline.py`)** \n",
    "   - Sucht pro Session ASD‑JSONs unter `speakers/spk*/centralcrops/*asd.json`, leitet UEM‑Fenster aus Frame‑IDs ab und baut Sprachsegmente mit `get_speaker_activity_segments`.\n",
    "   - Führt dann ein Agglomerativ‑Clustering mit k = Anzahl GT‑Cluster durch und bewertet F1/ARI gegen `labels/speakertocluster.json`.\n",
    "   - Schreibt Predictions nach `data-bin/dev/output_asdoverlap/<session>/speakertocluster_asdoverlap.json` und fasst Metriken über Sessions hinweg zusammen.\n",
    "\n",
    "2. **Seating-basierte Baseline-Evaluation (`eval_seating.py`)**\n",
    "   - Liest pro Session `labels/speakertocluster.json` und verschiedene sitzbasierte Heuristik‑Outputs (`speakertocluster_seat_neighbors.json`, `..._opposites.json`, `..._halves.json`, `..._distcomponents.json`, `..._distk2.json`).\n",
    "   - Berechnet F1 und ARI via `pairwise_f1_score` und `adjusted_rand_score` und gibt per‑Session sowie globale Mittelwerte aus.\n",
    "\n",
    "3. **Video-Cluster-Quality (`eval_video_cluster_quality.py`)**\n",
    "   - Lädt für jede Session `A.npy` (Seat‑Similaritätsmatrix) und `asdseatmatching.json`, konstruiert daraus mehrere Sitz‑Cluster‑Varianten (Nachbarn, Hälften, Opposites, hierarchisches k=2).\n",
    "   - Mappt Sitz‑Cluster via ASD‑Matching auf Speaker‑Cluster, baut aus `A` eine Similaritätsmatrix im Sprecherraum und bewertet diese Cluster sowohl über interne Metriken (Silhouette, within‑vs‑between‑Quality) als auch gegen GT (F1/ARI), falls vorhanden.\n",
    "   - Schreibt eine zusammenfassende JSON‑Datei `output/videovs_asd_eval.json` und optional eine TSV‑Übersicht pro Session.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb2581-66f7-48c4-9a29-70dc0acb63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper für die Evaluationsskripte (ASD-Overlap, Seating, Video-Qualität)\n",
    "\n",
    "import eval_asd_overlap_baseline as eval_asd\n",
    "import eval_seating as eval_seat\n",
    "import eval_video_cluster_quality as eval_vid\n",
    "\n",
    "\n",
    "def run_asd_overlap_baseline():\n",
    "    \"\"\"\n",
    "    Run the ASD-overlap baseline evaluation over all dev sessions.\n",
    "\n",
    "    Internally this:\n",
    "      - discovers all 'data-bin/dev/session*' folders,\n",
    "      - runs overlap-based clustering per session,\n",
    "      - writes predictions under 'data-bin/dev/output_asdoverlap',\n",
    "      - prints per-session and global F1/ARI scores. [file:4]\n",
    "    \"\"\"\n",
    "    eval_asd.main()  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "def run_seating_baseline_eval():\n",
    "    \"\"\"\n",
    "    Evaluate seating-based heuristics (neighbors, opposites, halves,\n",
    "    distance components, distk2) against GT speaker clusters. [file:5]\n",
    "    \"\"\"\n",
    "    eval_seat.main()  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "def run_video_cluster_quality_eval():\n",
    "    \"\"\"\n",
    "    Evaluate video-based seat similarity A.npy vs ASD overlap and GT clusters.\n",
    "\n",
    "    The script:\n",
    "      - iterates over 'data-bin/dev/session*',\n",
    "      - runs ASD-overlap baseline per session,\n",
    "      - evaluates several seat-based clusterings in the speaker space,\n",
    "      - writes JSON and TSV summaries to 'data-bin/output'. [file:6]\n",
    "    \"\"\"\n",
    "    eval_vid.main()  # type: ignore[attr-defined]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c4d08-f04f-4ce9-90ef-f0b650552157",
   "metadata": {},
   "source": [
    "## Visualisierung der Gaze- und Cluster-Statistiken (`plots_py.py` und Outputs)\n",
    "\n",
    "Das Skript `plots_py.py` liest aggregierte JSON‑Files wie `speaker_clustering_eval.json`, `speaker_distance_matrices.json` und ggf. zusätzliche Zusammenfassungen, um verschiedene Analyseplots zu erzeugen.\n",
    "Typische Beispiele sind Histogramme der Gaze‑basierten Distanzen (z. B. „ALL SESSIONS | distance_seat_gaze“) und Correct‑Rate‑Kurven in Abhängigkeit von einem Distanz‑Schwellwert („Correct‑Rate vs Threshold | distance_spectral_similarity“), wie in den beigefügten Abbildungen zu sehen.\n",
    "\n",
    "Die dafür genutzten JSON‑Strukturen enthalten:\n",
    "\n",
    "- pro Session: Sprecherreihenfolge, Distanzmatrizen und per‑Approach‑Clustering‑Metriken,\n",
    "- globale Auswertungen über alle Sessions, etwa mittlere ARI/F1‑Werte je Ansatz (Baseline‑Distanzen, Agglo‑Varianten, spektrale Gaze‑Cluster) und Binned‑Statistiken für Gaze‑Distanzen versus GT‑Clusterzugehörigkeit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9203ba-6a6e-4e63-966a-8dbbddc901e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Laden der globalen Speaker-Distance- und Clustering-Evals für Plotting \n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def load_global_distance_and_eval():\n",
    "    \"\"\"\n",
    "    Load the global distance and clustering evaluation JSONs that are\n",
    "    typically used for plotting. [file:3][file:9][file:10][file:14]\n",
    "    \"\"\"\n",
    "    dist_path = OUTPUT_GAZE_PLOTS_SPECTRAL / \"speaker_distance_matrices.json\"\n",
    "    eval_path = OUTPUT_GAZE_PLOTS_SPECTRAL / \"speaker_clustering_eval.json\"\n",
    "    summary_path = OUTPUT_GAZE_PLOTS_SPECTRAL / \"summary.json\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    if dist_path.exists():\n",
    "        with dist_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data[\"distance_matrices\"] = json.load(f)\n",
    "\n",
    "    if eval_path.exists():\n",
    "        with eval_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data[\"clustering_eval\"] = json.load(f)\n",
    "\n",
    "    if summary_path.exists():\n",
    "        with summary_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data[\"summary\"] = json.load(f)\n",
    "\n",
    "    print(\"Loaded keys:\", list(data.keys()))\n",
    "    return data\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37a086-ef94-485b-845e-b81ced17a37e",
   "metadata": {},
   "source": [
    "## End-to-End-Demonstration: Mini-Pipeline-Run für eine Session\n",
    "\n",
    "In diesem Abschnitt wird eine typische Ausführungsreihenfolge für Subtask 5 demonstriert:\n",
    "\n",
    "1. Zeit-/Semantik-/Hybrid‑Clustering für alle Dev‑Sessions mit `main.py` oder wahlweise nur eine Session über den Wrapper `run_time_semantic_hybrid_clustering`.\n",
    "2. Gaze‑basiertes Post‑Processing mit `post_step.main` zur Erzeugung von Distanzmatrizen, Agglo‑ und spektralen Gaze‑Clustern sowie globalen JSON‑Summaries.\n",
    "3. Optional: ASD‑Overlap‑Baseline, Seating‑Baselines und Video‑Cluster‑Quality‑Eval zum Vergleich der Video‑basierten Ansätze mit Audio/ASD‑basierten Verfahren.\n",
    "4. Visualisierung der Resultate (z. B. Distanz‑Histogramme und Correct‑Rate‑Kurven) mit `plots_py.py` und den geladenen JSON‑Summaries.\n",
    "\n",
    "Wo konkrete Datenpfade nötig sind, werden Platzhalter wie `\"/path/to/chime/data\"` verwendet; diese müssten im Challenge‑Setup an die lokale Ordnerstruktur angepasst werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff0ab0-8e50-4e56-90f9-b4a93aef96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Pipeline-Run-Beispiel\n",
    "\n",
    "def run_full_video_pipeline_for_dev():\n",
    "    \"\"\"\n",
    "    Demonstration of an end-to-end run for the dev set:\n",
    "\n",
    "    1. Time/semantic/hybrid clustering via main.py.\n",
    "    2. Gaze-based post-processing via post_step.py.\n",
    "    3. Optional ASD overlap / seating / video quality evals.\n",
    "    4. Load global JSONs for subsequent plotting. [file:2][file:3][file:4][file:5][file:6][file:8][file:9][file:10][file:14]\n",
    "    \"\"\"\n",
    "    # 1) Conversation clustering (time / semantic / hybrid).\n",
    "    run_time_semantic_hybrid_clustering(\n",
    "        session_glob=str(DEV_SESSIONS_ROOT / \"session*\"),\n",
    "        output_dirname=\"output_semantic\",\n",
    "        max_length=15,\n",
    "    )\n",
    "\n",
    "    # 2) Gaze-basiertes Post-Processing (Seat/Gaze-Distanzen, Agglo & Spectral). \n",
    "    run_post_step(\n",
    "        output_root=OUTPUT_GAZE_ROOT,\n",
    "        labels_root=DEV_SESSIONS_ROOT,\n",
    "    )\n",
    "\n",
    "    # 3) Optionale weitere Evals zum Vergleich der Videoansätze.\n",
    "    # Achtung: eval_asd_overlap_baseline und eval_seating arbeiten intern\n",
    "    # mit TEAMC_DATA_BIN/TEAMC-spezifischen Konstanten; \n",
    "    #\n",
    "    # run_asd_overlap_baseline()\n",
    "    # run_seating_baseline_eval()\n",
    "    # run_video_cluster_quality_eval()\n",
    "\n",
    "    # 4) Globale JSONs laden, z.B. für Plotting. \n",
    "    data = load_global_distance_and_eval()\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71c1b7-b429-4078-ae72-d8a91ef480b7",
   "metadata": {},
   "source": [
    "## Hinweise zur Evaluation & Weiterarbeit\n",
    "\n",
    "Die erzeugten Outputs dienen in der CHiME‑9‑Pipeline als Gesprächs‑Clusterings, die weiter in ASR‑ oder Diarisations‑Komponenten eingespeist werden können (z. B. zur Sprecher‑Weisung der Audio‑Kanäle oder für turn‑basierte Transkript‑Analyse).\n",
    "Die JSON‑Formate `speakertocluster_*.json` folgen dabei konsistent dem Schema `{\"spk0\": 0, \"spk1\": 1, ...}`, was eine einfache Integration in nachgelagerte Tools ermöglicht.\n",
    "Für Anpassungen an andere Sessions oder Konfigurationen bieten sich folgende Stellschrauben an:\n",
    "\n",
    "- **Session-Auswahl:** Änderung des Glob‑Patterns in `run_time_semantic_hybrid_clustering` (z. B. `data-bin/dev_central_videos/session*`) oder explizite Liste von Session‑Ordnern. \n",
    "- **Datenwurzeln:** Anpassung von `PROJECT_ROOT` und `DATABIN_ROOT` an lokale Pfade bzw. andere Team‑Verzeichnisse (z. B. `team_a`, `team_b`). \n",
    "- **Gaze-Parameter:** Die Konstanten `YAWSIGN`, `GAZE_MIN_SAMPLES`, `BASE_GAZE_TOL_DEG`, `MAX_GAZE_TOL_DEG`, `WMUTUAL` und `WONEWAY` steuern, wie stark und wie tolerant Gaze in die Distanzmatrizen einfließt; Änderungen sollten jedoch direkt in `post_step.py` erfolgen und sorgfältig evaluiert werden.\n",
    "- **Spektral-Parameter:** `SPECTRAL_KMIN`, `SPECTRAL_KMAX`, `SPECTRAL_KNN` und `SPECTRAL_GAMMA` definieren k‑Suche, kNN‑Sparsifizierung und Graph‑Schärfung für die spektralen Gaze‑Cluster.\n",
    "\n",
    "Für weiterführende Experimente kann das Notebook als zentraler Einstiegspunkt dienen, um neue Varianten (z. B. modifizierte Similaritätsdefinitionen, alternative k‑Auswahlstrategien oder zusätzliche Video‑Features) prototypisch zu testen, während die bestehende, challenge‑konforme Logik der Skripte unangetastet bleibt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
