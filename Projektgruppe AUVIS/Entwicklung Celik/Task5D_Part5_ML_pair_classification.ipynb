{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1f8fa7",
   "metadata": {},
   "source": [
    "# Task 5D – Teil 5: ML-basierte Gesprächszuordnung (Pair-Klassifikation)\n",
    "\n",
    "**Umfang dieses Notebooks (Teil 5):**\n",
    "\n",
    "1. Laden der vorhandenen Embeddings (`.npz`) und Metadaten (`.parquet`) inklusive Cluster-Informationen  \n",
    "2. Konstruktion eines Paar-Datasets: Bildung von positiven und negativen Sprecherpaaren innerhalb einer Session  \n",
    "3. Extraktion relevanter Merkmale (u. a. Cosine Similarity, zeitlicher Abstand, Sprecherwechsel, Überlappung, Textlänge)  \n",
    "4. Training klassischer ML-Modelle (Logistische Regression, Random Forest, optional SVM) zur Klassifikation der Paarzugehörigkeit  \n",
    "5. Evaluation anhand geeigneter Metriken (Accuracy, Precision, Recall, F1, ROC-AUC, Konfusionsmatrix)  \n",
    "6. Analyse der Feature-Bedeutung und Fehlklassifikationen zur Interpretation der Ergebnisse  \n",
    "7. Diskussion der Integration dieses Ansatzes in die bestehende Task-5D-Pipeline und Vergleich mit dem distanzbasierten Clustering aus Teil 4\n",
    "\n",
    "> **Hinweis:** Dieses Notebook erweitert die Kernkomponente des Task-5D-Prototyps um einen ML-basierten Klassifikationsansatz. Ziel ist es, die Robustheit der Gesprächstrennung zu erhöhen, indem semantische und zeitliche Merkmale kombiniert und direkt zur Vorhersage der Gesprächszugehörigkeit genutzt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd74aa",
   "metadata": {},
   "source": [
    "## 1. Imports, Reproduzierbarkeit & Setup\n",
    "\n",
    "In diesem Abschnitt werden die benötigten Bibliotheken geladen, Zufallssamen gesetzt sowie Pandas-Optionen für die Ausgabe definiert.  \n",
    "Darüber hinaus erfolgt die Basiskonfiguration (Pfadangaben, Sampling-Parameter).  \n",
    "Standardmäßig werden die Daten per **Auto-Discovery** gesucht, können bei Bedarf aber auch manuell überschrieben werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bcb33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/ercel001/AUVIS/task5D_ml_prototype\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) Imports, Reproduzierbarkeit & Setup\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- System / Utility ---\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# --- Numerik & Datenverarbeitung ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Fortschrittsbalken ---\n",
    "from tqdm.auto import tqdm   # für dynamische Fortschrittsanzeigen im Notebook\n",
    "\n",
    "# --- Machine Learning (Scikit-Learn) ---\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold   # gruppenbasierte Splits nach Session\n",
    "from sklearn.pipeline import Pipeline                               # Verarbeitungsketten (z. B. Scaling + Modell)\n",
    "from sklearn.preprocessing import StandardScaler                    # Standardisierung numerischer Features\n",
    "from sklearn.metrics import (                                       # gängige Klassifikationsmetriken\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression                 # Logistische Regression\n",
    "from sklearn.ensemble import RandomForestClassifier                 # Random Forest\n",
    "from sklearn.svm import SVC                                         # Support Vector Machine (optional)\n",
    "\n",
    "# --- Visualisierung ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproduzierbarkeit & Anzeigeoptionen\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Zufalls-Seed für Numpy -> sorgt für reproduzierbare Ergebnisse\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Pandas-Optionen für bessere Übersicht im Notebook\n",
    "pd.set_option(\"display.max_colwidth\", 200)   # volle Anzeige langer Texte\n",
    "pd.set_option(\"display.width\", 160)          # max. Zeilenbreite\n",
    "pd.set_option(\"display.max_columns\", 120)    # max. Spaltenanzahl\n",
    "\n",
    "# Warnungen unterdrücken (z. B. sklearn-Deprecations), um saubere Ausgabe zu erhalten\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Verzeichnis-Setup\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    \"\"\"\n",
    "    Legt ein Verzeichnis (inkl. Unterordnern) an, falls es noch nicht existiert.\n",
    "    Entspricht dem Verhalten von 'mkdir -p'.\n",
    "    \"\"\"\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Projekt-Root (einheitlich mit Notebook 2–4 setzen!)\n",
    "PROJECT_ROOT = Path.home() / \"AUVIS/task5D_ml_prototype\"\n",
    "\n",
    "# Unterordner für Ergebnisse von Teil 5 (Pair-Dataset)\n",
    "DERIVED_DIR = PROJECT_ROOT / \"data\" / \"derived\" / \"part5_pairs\"\n",
    "ensure_dir(DERIVED_DIR)\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebcd59",
   "metadata": {},
   "source": [
    "## 2. Auto-Discovery von Dateien\n",
    "\n",
    "In diesem Schritt werden die benötigten Dateien automatisch gesucht:\n",
    "\n",
    "- **Embeddings (.npz):** enthalten `utt_ids` und `embeddings`  \n",
    "- **Metadaten (.parquet):** enthalten u. a. `utt_id`, `speaker_id`, `session_id`, Zeitstempel (`start`, `end`) sowie die Cluster-Zugehörigkeit\n",
    "\n",
    "> Falls keine passenden Dateien gefunden werden, können die Pfade im nächsten Abschnitt manuell gesetzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22caf10d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Embeddings (.npz):\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/embeddings/dev_embeddings.npz\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/embeddings/train_embeddings.npz\n",
      "\n",
      "Gefundene Metadaten (.parquet):\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/prepared/dev_utterances.parquet\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/prepared/dev_utterances_multisession.parquet\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/prepared/train_utterances.parquet\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/prepared/train_utterances_multisession.parquet\n",
      "\n",
      "Gefundene speaker_to_cluster.json:\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_132/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_133/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_134/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_135/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_136/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_137/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_138/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_139/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_140/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_141/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_40/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_41/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_42/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_43/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_44/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_48/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_49/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_50/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_51/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_52/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_53/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_54/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_55/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_56/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_57/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_00/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_01/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_02/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_03/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_04/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_100/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_101/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_102/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_103/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_104/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_105/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_106/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_22/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_23/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_24/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_25/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_26/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_27/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_28/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_29/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_30/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_45/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_46/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_47/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_58/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_59/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_60/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_61/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_62/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_63/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_64/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_65/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_66/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_67/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_68/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_69/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_70/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_71/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_72/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_83/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_84/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_85/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_86/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_87/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_88/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_89/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_90/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_91/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_92/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_93/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_94/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_95/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_96/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_97/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_98/labels/speaker_to_cluster.json\n",
      "  - /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train/train/session_99/labels/speaker_to_cluster.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) Auto-Discovery von Dateien (mit Duplikat-Filter)\n",
    "# ============================================================\n",
    "\n",
    "SEARCH_DIRS = [\n",
    "    PROJECT_ROOT / \"data\",        # reicht aus – keine Überlappungen nötig\n",
    "    Path(\"/mnt/data\"),\n",
    "]\n",
    "\n",
    "def find_files(extensions=(\"npz\", \"parquet\", \"json\")):\n",
    "    found = {\"npz\": [], \"parquet\": [], \"json\": []}\n",
    "\n",
    "    for d in SEARCH_DIRS:\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for ext in extensions:\n",
    "            for p in d.rglob(f\"*.{ext}\"):\n",
    "                if ext == \"npz\" and \"emb\" in p.name.lower():\n",
    "                    found[\"npz\"].append(p)\n",
    "                elif ext == \"parquet\" and (\"utterances\" in p.name.lower()):\n",
    "                    found[\"parquet\"].append(p)\n",
    "                elif ext == \"json\" and \"speaker_to_cluster\" in p.name.lower():\n",
    "                    found[\"json\"].append(p)\n",
    "\n",
    "    # Duplikate entfernen & sortieren\n",
    "    for k in found:\n",
    "        found[k] = sorted(set(found[k]))\n",
    "    return found\n",
    "\n",
    "found = find_files()\n",
    "\n",
    "print(\"Gefundene Embeddings (.npz):\")\n",
    "for p in found[\"npz\"]:\n",
    "    print(\"  -\", p)\n",
    "\n",
    "print(\"\\nGefundene Metadaten (.parquet):\")\n",
    "for p in found[\"parquet\"]:\n",
    "    print(\"  -\", p)\n",
    "\n",
    "print(\"\\nGefundene speaker_to_cluster.json:\")\n",
    "for p in found[\"json\"]:\n",
    "    print(\"  -\", p)\n",
    "\n",
    "def pick_latest(paths):\n",
    "    if not paths:\n",
    "        return None\n",
    "    return max(paths, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "# Standard-Dateien (Fallback)\n",
    "DEFAULT_EMB_NPZ = pick_latest(found[\"npz\"])\n",
    "DEFAULT_META_PARQUET = pick_latest(found[\"parquet\"])\n",
    "DEFAULT_STC_JSON = pick_latest(found[\"json\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cfeae",
   "metadata": {},
   "source": [
    "## 3. Manuelle Konfiguration (optional)\n",
    "\n",
    "Standardmäßig werden die zuletzt gefundenen Dateien aus der Auto-Discovery genutzt.  \n",
    "Falls diese nicht passen, können hier die Pfade sowie zentrale Parameter manuell gesetzt werden.  \n",
    "(Dieser Abschnitt kann bei funktionierender Auto-Discovery übersprungen oder auskommentiert werden.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d683d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfiguration (aktiv):\n",
      "  EMB_NPZ_FILE= /home/ercel001/AUVIS/task5D_ml_prototype/data/embeddings/dev_embeddings.npz\n",
      "  META_PARQUET_FILE= /home/ercel001/AUVIS/task5D_ml_prototype/data/prepared/dev_utterances_multisession.parquet\n",
      "  SPEAKER_TO_CLUSTER_JSON= /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev/dev/session_137/labels/speaker_to_cluster.json\n",
      "  MAX_POS_PAIRS_PER_UTT= 2\n",
      "  NEGATIVE_MULTIPLIER= 2\n",
      "  MAX_TOTAL_PAIRS= 300000\n",
      "  TIME_WINDOW_S= None\n",
      "  USE_SVM= False\n",
      "  N_ESTIMATORS_RF= 300\n",
      "  N_SPLITS_GROUPKFOLD= 5\n",
      "  TEST_SIZE= 0.2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) Manuelle Konfiguration (optional)\n",
    "# ============================================================\n",
    "\n",
    "# Standardfall: Auto-Discovery-Defaults verwenden.\n",
    "# → Wenn du manuell überschreiben willst, setze einfach unten einen eigenen Pfad.\n",
    "EMB_NPZ_FILE = DEFAULT_EMB_NPZ            # z.B. Path(\"/…/train_embeddings.npz\")\n",
    "META_PARQUET_FILE = DEFAULT_META_PARQUET  # z.B. Path(\"/…/train_utterances.parquet\")\n",
    "SPEAKER_TO_CLUSTER_JSON = DEFAULT_STC_JSON  # optional; nur nötig, falls im Parquet kein Cluster steht\n",
    "\n",
    "# Parameter für die Paarbildung / Modelle (bei Bedarf anpassen)\n",
    "MAX_POS_PAIRS_PER_UTT = 2\n",
    "NEGATIVE_MULTIPLIER = 2\n",
    "MAX_TOTAL_PAIRS = 300_000\n",
    "TIME_WINDOW_S = None\n",
    "\n",
    "USE_SVM = False\n",
    "N_ESTIMATORS_RF = 300\n",
    "N_SPLITS_GROUPKFOLD = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(\"Konfiguration (aktiv):\")\n",
    "print(\"  EMB_NPZ_FILE=\", EMB_NPZ_FILE)\n",
    "print(\"  META_PARQUET_FILE=\", META_PARQUET_FILE)\n",
    "print(\"  SPEAKER_TO_CLUSTER_JSON=\", SPEAKER_TO_CLUSTER_JSON)\n",
    "print(\"  MAX_POS_PAIRS_PER_UTT=\", MAX_POS_PAIRS_PER_UTT)\n",
    "print(\"  NEGATIVE_MULTIPLIER=\", NEGATIVE_MULTIPLIER)\n",
    "print(\"  MAX_TOTAL_PAIRS=\", MAX_TOTAL_PAIRS)\n",
    "print(\"  TIME_WINDOW_S=\", TIME_WINDOW_S)\n",
    "print(\"  USE_SVM=\", USE_SVM)\n",
    "print(\"  N_ESTIMATORS_RF=\", N_ESTIMATORS_RF)\n",
    "print(\"  N_SPLITS_GROUPKFOLD=\", N_SPLITS_GROUPKFOLD)\n",
    "print(\"  TEST_SIZE=\", TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fc6ab",
   "metadata": {},
   "source": [
    "## 4. Daten laden & prüfen\n",
    "\n",
    "In diesem Abschnitt werden die **Embeddings** (`.npz`) und die **Metadaten** (`.parquet`) geladen.  \n",
    "Erwartet werden mindestens folgende Felder (ggf. unter abweichenden Spaltennamen, die wir robust abfangen):\n",
    "\n",
    "- `utt_id` (eindeutige ID einer Äußerung)  \n",
    "- `speaker_id`  \n",
    "- `session_id`  \n",
    "- Zeitfelder: `start`, `end` (oder Varianten)  \n",
    "- `text`  \n",
    "- **Konversations-Label**: z. B. `cluster`, `conv_id` (falls nicht vorhanden, wird – wenn verfügbar – `speaker_to_cluster.json` gejoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b7db85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadaten geladen: (8561, 7)\n",
      "Embeddings geladen: (8561, 768)\n",
      "Gefiltert: 8561 → 8561 (nach Schnittmenge mit Embeddings)\n",
      "Finale Metadaten: (8561, 8)\n",
      "Vorhandene Spalten (erste 30): ['cluster', 'end_s', 'session_id', 'speaker_id', 'start_s', 'text', 'text_len', 'utt_id']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Daten laden & prüfen\n",
    "# ============================================================\n",
    "\n",
    "def load_embeddings(npz_path: Path):\n",
    "    \"\"\"\n",
    "    Lädt Embeddings (.npz) und gibt (utt_ids, embeddings) zurück.\n",
    "    Erwartet:\n",
    "      - Schlüssel mit 'utt' oder 'id' → Äußerungs-IDs\n",
    "      - Schlüssel mit 'emb'          → Embedding-Matrix\n",
    "    \"\"\"\n",
    "    if npz_path is None or not Path(npz_path).exists():\n",
    "        raise FileNotFoundError(f\"Embeddings-Datei nicht gefunden: {npz_path}\")\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    id_keys = [k for k in data.files if \"utt\" in k or \"id\" in k]\n",
    "    emb_keys = [k for k in data.files if \"emb\" in k]\n",
    "\n",
    "    if not id_keys or not emb_keys:\n",
    "        raise KeyError(f\"Unerwartete Schlüssel in {npz_path}. Gefunden: {data.files}\")\n",
    "\n",
    "    utt_ids = data[id_keys[0]]\n",
    "    X = data[emb_keys[0]]\n",
    "\n",
    "    assert len(utt_ids) == len(X), \"Länge von utt_ids und embeddings passt nicht\"\n",
    "    return utt_ids.astype(str), X.astype(np.float32)\n",
    "\n",
    "\n",
    "def parse_time_to_seconds(x):\n",
    "    \"\"\"Konvertiert verschiedene Zeitformate in Sekunden.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.integer, np.floating)):\n",
    "        return float(x)\n",
    "\n",
    "    s = str(x)\n",
    "    try:\n",
    "        td = pd.to_timedelta(s)\n",
    "        return td.total_seconds()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        parts = s.split(':')\n",
    "        if len(parts) == 3:\n",
    "            h, m, sec = parts\n",
    "            return int(h)*3600 + int(m)*60 + float(sec)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def robust_column(df: pd.DataFrame, candidates, required: bool = True):\n",
    "    \"\"\"\n",
    "    Gibt den ersten Spaltennamen zurück, der im DataFrame vorhanden ist.\n",
    "    Falls required=True und keine Treffer → KeyError.\n",
    "    \"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"Erforderliche Spalte nicht gefunden. Kandidaten: {candidates}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Metadaten laden ---\n",
    "if META_PARQUET_FILE is None or not Path(META_PARQUET_FILE).exists():\n",
    "    raise FileNotFoundError(f\"Metadaten-Parquet nicht gefunden: {META_PARQUET_FILE}\")\n",
    "\n",
    "df_meta = pd.read_parquet(META_PARQUET_FILE)\n",
    "print(\"Metadaten geladen:\", df_meta.shape)\n",
    "\n",
    "# Spalten robust identifizieren\n",
    "utt_col     = robust_column(df_meta, [\"utt_id\", \"utt\", \"utterance_id\", \"id\"])\n",
    "speaker_col = robust_column(df_meta, [\"speaker_id\", \"spk\", \"speaker\"])\n",
    "session_col = robust_column(df_meta, [\"session_id\", \"session\", \"sess_id\"])\n",
    "start_col   = robust_column(df_meta, [\"start\", \"start_time\", \"ts_start\", \"begin\"], required=False)\n",
    "end_col     = robust_column(df_meta, [\"end\", \"end_time\", \"ts_end\", \"stop\"], required=False)\n",
    "text_col    = robust_column(df_meta, [\"text\", \"transcript\", \"utt_text\"], required=False)\n",
    "\n",
    "# Cluster-Spalte bestimmen oder speaker_to_cluster.json joinen\n",
    "cluster_col = None\n",
    "for cand in [\"cluster\", \"conv_id\", \"conversation_id\", \"cluster_id\", \"conv\"]:\n",
    "    if cand in df_meta.columns:\n",
    "        cluster_col = cand\n",
    "        break\n",
    "\n",
    "if cluster_col is None and SPEAKER_TO_CLUSTER_JSON and Path(SPEAKER_TO_CLUSTER_JSON).exists():\n",
    "    with open(SPEAKER_TO_CLUSTER_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        stc = json.load(f)\n",
    "    rows = []\n",
    "    if isinstance(stc, dict):\n",
    "        for sess_k, mapping in stc.items():\n",
    "            if isinstance(mapping, dict):\n",
    "                for spk_k, cl in mapping.items():\n",
    "                    rows.append({\"session_id\": str(sess_k), \"speaker_id\": str(spk_k), \"cluster\": cl})\n",
    "    stc_df = pd.DataFrame(rows)\n",
    "    if not stc_df.empty:\n",
    "        stc_df[\"session_id\"] = stc_df[\"session_id\"].astype(str)\n",
    "        stc_df[\"speaker_id\"] = stc_df[\"speaker_id\"].astype(str)\n",
    "        df_meta[session_col] = df_meta[session_col].astype(str)\n",
    "        df_meta[speaker_col] = df_meta[speaker_col].astype(str)\n",
    "        df_meta = df_meta.merge(stc_df, left_on=[session_col, speaker_col],\n",
    "                                right_on=[\"session_id\", \"speaker_id\"], how=\"left\")\n",
    "        cluster_col = \"cluster\"\n",
    "\n",
    "if cluster_col is None:\n",
    "    raise KeyError(\"Keine Konversations-/Cluster-Spalte gefunden. \"\n",
    "                   \"Bitte sicherstellen, dass Teil 2/4 die Cluster im Parquet enthalten \"\n",
    "                   \"oder 'SPEAKER_TO_CLUSTER_JSON' angeben.\")\n",
    "\n",
    "# Zeitspalten konvertieren\n",
    "if start_col:\n",
    "    df_meta[\"start_s\"] = df_meta[start_col].map(parse_time_to_seconds)\n",
    "if end_col:\n",
    "    df_meta[\"end_s\"] = df_meta[end_col].map(parse_time_to_seconds)\n",
    "\n",
    "# Textlänge\n",
    "if text_col:\n",
    "    df_meta[\"text_len\"] = df_meta[text_col].astype(str).map(lambda s: len(s.split()))\n",
    "\n",
    "# --- Embeddings laden ---\n",
    "if EMB_NPZ_FILE is None or not Path(EMB_NPZ_FILE).exists():\n",
    "    raise FileNotFoundError(f\"Embeddings-Datei nicht gefunden: {EMB_NPZ_FILE}\")\n",
    "\n",
    "utt_ids, X = load_embeddings(Path(EMB_NPZ_FILE))\n",
    "print(\"Embeddings geladen:\", X.shape)\n",
    "\n",
    "# Schnittmenge bilden\n",
    "before = len(df_meta)\n",
    "utt_set = set(utt_ids.tolist())\n",
    "df_meta = df_meta[df_meta[utt_col].astype(str).isin(utt_set)].copy()\n",
    "after = len(df_meta)\n",
    "\n",
    "print(f\"Gefiltert: {before} → {after} (nach Schnittmenge mit Embeddings)\")\n",
    "\n",
    "# Sortieren nach Session + Zeit\n",
    "df_meta = df_meta.sort_values([session_col, \"start_s\" if \"start_s\" in df_meta.columns else utt_col]).reset_index(drop=True)\n",
    "\n",
    "print(\"Finale Metadaten:\", df_meta.shape)\n",
    "print(\"Vorhandene Spalten (erste 30):\", sorted(df_meta.columns.tolist())[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8532b7",
   "metadata": {},
   "source": [
    "## 5. Konstruktion des Paar-Datasets\n",
    "\n",
    "Für jede **Session** werden Paare `(i, j)` von Äußerungen gebildet:  \n",
    "\n",
    "- **Positive Paare:** beide Äußerungen im gleichen Konversations-Cluster  \n",
    "- **Negative Paare:** Äußerungen aus unterschiedlichen Clustern derselben Session  \n",
    "\n",
    "Um Rechenaufwand zu kontrollieren, nutzen wir Sampling-Parameter:  \n",
    "- `MAX_POS_PAIRS_PER_UTT`: max. Anzahl positiver Paare pro Äußerung (zufällig gewählt)  \n",
    "- `NEGATIVE_MULTIPLIER`: Faktor für negative Paare je positivem Paar  \n",
    "- `TIME_WINDOW_S`: optionaler Zeitfilter (|Δt| ≤ Zeitfenster)\n",
    "\n",
    "**Features pro Paar:**  \n",
    "- `cos_sim`, `time_gap_s` (+ `log_time_gap`), `overlap`, `same_speaker`  \n",
    "- Dauer (`dur_i`, `dur_j`, `duration_diff`)  \n",
    "- Textlängen (`len_i`, `len_j`, `len_diff`)  \n",
    "- `turn_distance` (Unterschied der Position im Gespräch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34920a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcd4fde84f84e92a37ac65325fa7527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sessions:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paar-Dataset: (17122, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>label</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>time_gap_s</th>\n",
       "      <th>log_time_gap</th>\n",
       "      <th>overlap</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>duration_i</th>\n",
       "      <th>duration_j</th>\n",
       "      <th>duration_diff</th>\n",
       "      <th>len_i</th>\n",
       "      <th>len_j</th>\n",
       "      <th>len_diff</th>\n",
       "      <th>turn_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12933</th>\n",
       "      <td>session_51_4_0022</td>\n",
       "      <td>session_51_4_0055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15843</th>\n",
       "      <td>session_55_2_0047</td>\n",
       "      <td>session_55_1_0030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>session_49_0_0005</td>\n",
       "      <td>session_49_0_0009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>session_141_3_0051</td>\n",
       "      <td>session_141_1_0047</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15842</th>\n",
       "      <td>session_55_2_0047</td>\n",
       "      <td>session_55_1_0019</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.061049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        u                   v  label   cos_sim  time_gap_s  log_time_gap  overlap  same_speaker  duration_i  duration_j  duration_diff  len_i  \\\n",
       "12933   session_51_4_0022   session_51_4_0055      1  0.075796         NaN           NaN        0             1         NaN         NaN            NaN      1   \n",
       "15843   session_55_2_0047   session_55_1_0030      1  0.021799         NaN           NaN        0             0         NaN         NaN            NaN     10   \n",
       "11364   session_49_0_0005   session_49_0_0009      1  0.111037         NaN           NaN        0             1         NaN         NaN            NaN      8   \n",
       "6911   session_141_3_0051  session_141_1_0047      1  0.165890         NaN           NaN        0             0         NaN         NaN            NaN      2   \n",
       "15842   session_55_2_0047   session_55_1_0019      1 -0.061049         NaN           NaN        0             0         NaN         NaN            NaN     10   \n",
       "\n",
       "       len_j  len_diff  turn_distance  \n",
       "12933      3        -2            193  \n",
       "15843      5         5            126  \n",
       "11364      6         2             59  \n",
       "6911       1         1             31  \n",
       "15842     11        -1            176  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: /home/ercel001/AUVIS/task5D_ml_prototype/data/derived/part5_pairs/pairs_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5) Konstruktion des Paar-Datasets\n",
    "# ============================================================\n",
    "\n",
    "# Mapping: Utterance-ID -> Index im Embedding-Array\n",
    "utt_to_idx = {str(u): i for i, u in enumerate(utt_ids)}\n",
    "\n",
    "def l2_normalize_rows(M: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normiert jede Zeile einer Matrix auf Länge 1 (L2-Norm).\n",
    "    Verhindert, dass Cosine Similarity von der Betragsgröße beeinflusst wird.\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(M, axis=1, keepdims=True) + 1e-9\n",
    "    return M / norms\n",
    "\n",
    "# Vorberechnete normalisierte Embeddings\n",
    "X_norm = l2_normalize_rows(X)\n",
    "\n",
    "def cos_sim_by_ids(u1: str, u2: str) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die Cosine Similarity zwischen zwei Utterances anhand ihrer IDs.\n",
    "    Gibt NaN zurück, falls eine ID fehlt.\n",
    "    \"\"\"\n",
    "    i1 = utt_to_idx.get(str(u1), None)\n",
    "    i2 = utt_to_idx.get(str(u2), None)\n",
    "    if i1 is None or i2 is None:\n",
    "        return np.nan\n",
    "    return float(np.dot(X_norm[i1], X_norm[i2]))\n",
    "\n",
    "def make_pairs_for_session(df_sess: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Erzeugt Paare (positive & negative) für eine Session und extrahiert Features.\n",
    "\n",
    "    Schritte:\n",
    "      1. Positive Paare: innerhalb desselben Clusters\n",
    "      2. Negative Paare: zufällig zwischen Clustern (gesteuert über NEGATIVE_MULTIPLIER)\n",
    "      3. Feature-Berechnung für jedes Paar (Ähnlichkeiten, Zeitabstände, etc.)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # Session sortieren nach Startzeit oder Index\n",
    "    order = df_sess.sort_values(\"start_s\" if \"start_s\" in df_sess.columns else df_sess.index).reset_index(drop=True)\n",
    "    order[\"turn_idx\"] = np.arange(len(order))  # Reihenfolge im Gespräch\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1) Positive Paare\n",
    "    # --------------------------------------------------------\n",
    "    for clust, df_c in order.groupby(cluster_col):\n",
    "        ids_c = df_c[utt_col].astype(str).tolist()\n",
    "        if len(ids_c) < 2:\n",
    "            continue\n",
    "        for u in ids_c:\n",
    "            partners = [v for v in ids_c if v != u]\n",
    "            if not partners:\n",
    "                continue\n",
    "            np.random.shuffle(partners)\n",
    "            partners = partners[:MAX_POS_PAIRS_PER_UTT]\n",
    "            for v in partners:\n",
    "                rows.append((u, v, 1))  # Label = 1 (gleiches Gespräch)\n",
    "\n",
    "    pos_count = sum(1 for _, _, y in rows if y == 1)\n",
    "    neg_target = pos_count * NEGATIVE_MULTIPLIER\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Negative Paare\n",
    "    # --------------------------------------------------------\n",
    "    df_by_cluster = {c: d for c, d in order.groupby(cluster_col)}\n",
    "    clusters = list(df_by_cluster.keys())\n",
    "    utts_by_cluster = {c: d[utt_col].astype(str).tolist() for c, d in df_by_cluster.items()}\n",
    "\n",
    "    # Hilfstabellen für schnelle Zugriffe\n",
    "    start_map = dict(zip(order[utt_col].astype(str), order[\"start_s\" if \"start_s\" in order.columns else utt_col]))\n",
    "    end_map   = dict(zip(order[utt_col].astype(str), order[\"end_s\" if \"end_s\" in order.columns else utt_col]))\n",
    "    spk_map   = dict(zip(order[utt_col].astype(str), order[speaker_col].astype(str)))\n",
    "    turn_map  = dict(zip(order[utt_col].astype(str), order[\"turn_idx\"]))\n",
    "    len_map   = dict(zip(order[utt_col].astype(str), order.get(\"text_len\", pd.Series([np.nan]*len(order))).fillna(np.nan)))\n",
    "\n",
    "    dur_map = {}\n",
    "    for u in order[utt_col].astype(str):\n",
    "        s = start_map.get(u, np.nan)\n",
    "        e = end_map.get(u, np.nan)\n",
    "        dur_map[u] = (e - s) if (isinstance(s, float) and isinstance(e, float)) else np.nan\n",
    "\n",
    "    neg_rows = []\n",
    "    attempts = 0\n",
    "    max_attempts = neg_target * 10 + 1000  # Sicherheitslimit\n",
    "    while len(neg_rows) < neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        if len(clusters) < 2:\n",
    "            break\n",
    "        c1, c2 = np.random.choice(clusters, size=2, replace=False)\n",
    "        u = np.random.choice(utts_by_cluster[c1])\n",
    "        v = np.random.choice(utts_by_cluster[c2])\n",
    "        if TIME_WINDOW_S is not None:\n",
    "            t1 = start_map.get(u, np.nan)\n",
    "            t2 = start_map.get(v, np.nan)\n",
    "            if (isinstance(t1, float) and isinstance(t2, float)) and abs(t1 - t2) > TIME_WINDOW_S:\n",
    "                continue\n",
    "        neg_rows.append((u, v, 0))  # Label = 0 (verschiedene Gespräche)\n",
    "\n",
    "    rows.extend(neg_rows)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) Feature-Berechnung\n",
    "    # --------------------------------------------------------\n",
    "    feats = []\n",
    "    for u, v, y in rows:\n",
    "        cs = cos_sim_by_ids(u, v)\n",
    "        t1, e1 = start_map.get(u, np.nan), end_map.get(u, np.nan)\n",
    "        t2, e2 = start_map.get(v, np.nan), end_map.get(v, np.nan)\n",
    "\n",
    "        time_gap = np.nan\n",
    "        overlap = 0\n",
    "        dur1 = np.nan\n",
    "        dur2 = np.nan\n",
    "        if isinstance(t1, float) and isinstance(t2, float):\n",
    "            time_gap = abs(t2 - t1)\n",
    "        if isinstance(t1, float) and isinstance(e1, float) and isinstance(t2, float) and isinstance(e2, float):\n",
    "            overlap = int((e1 > t2) and (e2 > t1))\n",
    "            dur1 = e1 - t1\n",
    "            dur2 = e2 - t2\n",
    "\n",
    "        same_spk = int(spk_map.get(u) == spk_map.get(v))\n",
    "\n",
    "        turn_diff = np.nan\n",
    "        if u in turn_map and v in turn_map:\n",
    "            turn_diff = abs(int(turn_map[u]) - int(turn_map[v]))\n",
    "\n",
    "        len1 = len_map.get(u, np.nan)\n",
    "        len2 = len_map.get(v, np.nan)\n",
    "\n",
    "        feats.append({\n",
    "            \"u\": u, \"v\": v, \"label\": y,\n",
    "            \"cos_sim\": cs,\n",
    "            \"time_gap_s\": time_gap,\n",
    "            \"log_time_gap\": math.log1p(time_gap) if isinstance(time_gap, float) else np.nan,\n",
    "            \"overlap\": overlap,\n",
    "            \"same_speaker\": same_spk,\n",
    "            \"duration_i\": dur1, \"duration_j\": dur2,\n",
    "            \"duration_diff\": (dur1 - dur2) if (isinstance(dur1, float) and isinstance(dur2, float)) else np.nan,\n",
    "            \"len_i\": len1, \"len_j\": len2,\n",
    "            \"len_diff\": (len1 - len2) if (isinstance(len1, (int, float)) and isinstance(len2, (int, float))) else np.nan,\n",
    "            \"turn_distance\": turn_diff,\n",
    "        })\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paarbildung über alle Sessions\n",
    "# ------------------------------------------------------------\n",
    "pairs_list = []\n",
    "total_pairs = 0\n",
    "for sess, df_sess in tqdm(df_meta.groupby(session_col), desc=\"Sessions\"):\n",
    "    df_pairs = make_pairs_for_session(df_sess)\n",
    "    if df_pairs.empty:\n",
    "        continue\n",
    "    pairs_list.append(df_pairs)\n",
    "    total_pairs += len(df_pairs)\n",
    "    if total_pairs >= MAX_TOTAL_PAIRS:\n",
    "        break\n",
    "\n",
    "pairs = pd.concat(pairs_list, ignore_index=True) if pairs_list else pd.DataFrame(columns=[\n",
    "    \"u\",\"v\",\"label\",\"cos_sim\",\"time_gap_s\",\"log_time_gap\",\"overlap\",\"same_speaker\",\n",
    "    \"duration_i\",\"duration_j\",\"duration_diff\",\"len_i\",\"len_j\",\"len_diff\",\"turn_distance\"\n",
    "])\n",
    "\n",
    "print(\"Paar-Dataset:\", pairs.shape)\n",
    "display(pairs.sample(min(5, len(pairs))) if len(pairs) else pairs)\n",
    "\n",
    "# Speicherung des Datasets\n",
    "out_pairs = PROJECT_ROOT / \"data\" / \"derived\" / \"part5_pairs\" / \"pairs_dataset.parquet\"\n",
    "out_pairs.parent.mkdir(parents=True, exist_ok=True)\n",
    "pairs.to_parquet(out_pairs, index=False)\n",
    "print(\"Gespeichert:\", out_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdefc9d",
   "metadata": {},
   "source": [
    "## 6. Train/Test-Splits (gruppenbasiert) & Modellierung\n",
    "\n",
    "Um **Leckagen** zu vermeiden, wird der Split **gruppenbasiert nach Session** durchgeführt.  \n",
    "Dadurch können keine Utterances derselben Session gleichzeitig im Training und Testset landen.  \n",
    "\n",
    "Wir trainieren:  \n",
    "- **Logistische Regression** (mit Standardisierung)  \n",
    "- **Random Forest** (ohne Standardisierung notwendig)  \n",
    "- optional **SVM** (mit Standardisierung)  \n",
    "\n",
    "Zusätzlich erfolgt eine **gruppenbasierte Cross-Validation (GroupKFold)** auf dem Trainingsset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700a12a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13822, 12), Test: (3300, 12)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6) Train/Test-Splits (gruppenbasiert) & Modellierung\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Mapping von Utterance-ID -> Session-ID\n",
    "# ------------------------------------------------------------\n",
    "utt2sess = dict(zip(df_meta[utt_col].astype(str), df_meta[session_col].astype(str)))\n",
    "\n",
    "# Session-Zuordnung für beide Utterances im Paar\n",
    "pairs[\"session_u\"] = pairs[\"u\"].map(utt2sess)\n",
    "pairs[\"session_v\"] = pairs[\"v\"].map(utt2sess)\n",
    "\n",
    "# Nur Paare behalten, die aus derselben Session stammen\n",
    "pairs = pairs[pairs[\"session_u\"] == pairs[\"session_v\"]].copy()\n",
    "pairs[\"session\"] = pairs[\"session_u\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Feature-Auswahl\n",
    "# ------------------------------------------------------------\n",
    "feature_cols = [\n",
    "    \"cos_sim\", \"time_gap_s\", \"log_time_gap\", \"overlap\", \"same_speaker\",\n",
    "    \"duration_i\", \"duration_j\", \"duration_diff\",\n",
    "    \"len_i\", \"len_j\", \"len_diff\", \"turn_distance\"\n",
    "]\n",
    "\n",
    "X_df = pairs[feature_cols].copy()\n",
    "y = pairs[\"label\"].astype(int).values\n",
    "groups = pairs[\"session\"].astype(str).values  # Sessions als Gruppen für Splits\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Vorverarbeitung: NaN & Inf behandeln\n",
    "# ------------------------------------------------------------\n",
    "X_df = X_df.replace([np.inf, -np.inf], np.nan)\n",
    "medians = X_df.median(numeric_only=True)  # Median je Spalte\n",
    "X_df = X_df.fillna(medians)               # fehlende Werte ersetzen\n",
    "Xn = X_df.values                          # numpy-Array für Modelle\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Gruppenbasierter Split in Train/Test\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    TEST_SIZE\n",
    "except NameError:\n",
    "    TEST_SIZE = 0.2  # Default, falls nicht gesetzt\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(Xn, y, groups))\n",
    "\n",
    "X_train, X_test = Xn[train_idx], Xn[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "groups_train = groups[train_idx]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e654f00",
   "metadata": {},
   "source": [
    "## 7) Modelle definieren & trainieren\n",
    "\n",
    "Wir verwenden je ein **Pipeline‑Objekt** pro Modell:\n",
    "\n",
    "- `LogisticRegression` (mit `class_weight='balanced'`)\n",
    "- `RandomForestClassifier` (mit `class_weight='balanced_subsample'`)\n",
    "- Optional: `SVC` (RBF‑Kernel, `probability=True`, `class_weight='balanced'`) – **vorsichtig** bei großen Datenmengen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6436e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Klassenverteilung: [    0 13822]\n",
      "Test-Klassenverteilung:  [   0 3300]\n",
      "[WARN] Trainset enthält nur eine Klasse – Split neu ziehen!\n",
      "Neue Verteilung: [    0 13822]\n",
      "\n",
      "=== Trainiere LogReg ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Trainiere \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFertig.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1253\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1251\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1261\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7) Modelle definieren & trainieren (mit Class-Balance-Check)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Defaults\n",
    "N_ESTIMATORS_RF = globals().get(\"N_ESTIMATORS_RF\", 200)\n",
    "USE_SVM = globals().get(\"USE_SVM\", False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: check label balance\n",
    "# ------------------------------------------------------------\n",
    "def has_two_classes(y):\n",
    "    return len(np.unique(y)) > 1\n",
    "\n",
    "print(\"Train-Klassenverteilung:\", np.bincount(y_train))\n",
    "print(\"Test-Klassenverteilung: \", np.bincount(y_test))\n",
    "\n",
    "if not has_two_classes(y_train):\n",
    "    print(\"[WARN] Trainset enthält nur eine Klasse – Split neu ziehen!\")\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "    gss = GroupShuffleSplit(n_splits=5, test_size=TEST_SIZE, random_state=42)\n",
    "    for train_idx, test_idx in gss.split(Xn, y, groups):\n",
    "        y_tmp = y[train_idx]\n",
    "        if has_two_classes(y_tmp):\n",
    "            X_train, X_test = Xn[train_idx], Xn[test_idx]\n",
    "            y_train, y_test = y_tmp, y[test_idx]\n",
    "            groups_train = groups[train_idx]\n",
    "            break\n",
    "    print(\"Neue Verteilung:\", np.bincount(y_train))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Pipelines\n",
    "# ------------------------------------------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "logreg = Pipeline([\n",
    "    (\"imputer\", imputer),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"imputer\", imputer),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=N_ESTIMATORS_RF,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "svm = None\n",
    "if USE_SVM:\n",
    "    svm = Pipeline([\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            probability=True,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "models = {\"LogReg\": logreg, \"RandomForest\": rf}\n",
    "if svm is not None:\n",
    "    models[\"SVM\"] = svm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training\n",
    "# ------------------------------------------------------------\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Trainiere {name} ===\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nFertig.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71deb9d9-5b0f-43e0-82fb-70f7c32808bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE_MULTIPLIER = 2\n",
      "TIME_WINDOW_S = None\n",
      "\n",
      "Cluster-Anzahl je Session:\n",
      "count    25.0\n",
      "mean      1.0\n",
      "std       0.0\n",
      "min       1.0\n",
      "25%       1.0\n",
      "50%       1.0\n",
      "75%       1.0\n",
      "max       1.0\n",
      "Name: cluster, dtype: float64\n",
      "cluster\n",
      "1    25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"NEGATIVE_MULTIPLIER =\", NEGATIVE_MULTIPLIER)\n",
    "print(\"TIME_WINDOW_S =\", TIME_WINDOW_S)\n",
    "\n",
    "# Cluster-Anzahl je Session\n",
    "cluster_counts = df_meta.groupby(session_col)[cluster_col].nunique()\n",
    "print(\"\\nCluster-Anzahl je Session:\")\n",
    "print(cluster_counts.describe())\n",
    "print(cluster_counts.value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51409b",
   "metadata": {},
   "source": [
    "## 5) Evaluation (Testset) & Cross‑Validation\n",
    "\n",
    "Wir berichten **Accuracy, Precision, Recall, F1, ROC‑AUC** und zeigen eine **Konfusionsmatrix**.  \n",
    "Zusätzlich führen wir eine **gruppenbasierte CV** (GroupKFold) auf dem Trainingsset durch, um die Stabilität zu prüfen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(name: str, model, X_tr, y_tr, X_te, y_te):\n",
    "    y_pred = model.predict(X_te)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_te)[:, 1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "    \n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_te, y_pred, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(y_te, y_proba) if y_proba is not None else np.nan\n",
    "    \n",
    "    print(f\"\\n[{name}] Test-Ergebnisse:\")\n",
    "    print(f\"  Accuracy : {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall   : {rec:.4f}\")\n",
    "    print(f\"  F1       : {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC  : {auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_te, y_pred, digits=3))\n",
    "    \n",
    "    cm = confusion_matrix(y_te, y_pred, labels=[0,1])\n",
    "    print(\"Konfusionsmatrix (Zeile=Ist, Spalte=Vorhersage):\\n\", cm)\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_te, y_proba)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "        plt.plot([0,1],[0,1], linestyle='--')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC-Kurve – {name}\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "for name, model in models.items():\n",
    "    evaluate_model(name, model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nGruppenbasierte Cross-Validation (nur Trainingsteil):\")\n",
    "gkf = GroupKFold(n_splits=N_SPLITS_GROUPKFOLD)\n",
    "for name, model in models.items():\n",
    "    scores = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train, y_train, groups=groups_train), 1):\n",
    "        mdl = model\n",
    "        mdl.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "        y_pred = mdl.predict(X_train[va_idx])\n",
    "        try:\n",
    "            y_proba = mdl.predict_proba(X_train[va_idx])[:, 1]\n",
    "            auc = roc_auc_score(y_train[va_idx], y_proba)\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        f1 = precision_recall_fscore_support(y_train[va_idx], y_pred, average=\"binary\", zero_division=0)[2]\n",
    "        scores.append((f1, auc))\n",
    "    f1_mean = float(np.nanmean([s[0] for s in scores]))\n",
    "    auc_mean = float(np.nanmean([s[1] for s in scores]))\n",
    "    print(f\"  {name}: mean F1={f1_mean:.4f}, mean ROC-AUC={auc_mean:.4f} (über {N_SPLITS_GROUPKFOLD} Folds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96db02b",
   "metadata": {},
   "source": [
    "## 6) Feature‑Bedeutung & Fehleranalyse\n",
    "\n",
    "Wir untersuchen, welche Merkmale besonders relevant sind (LR‑Koeffizienten, RF‑Feature‑Importances) und betrachten **Fehlklassifikationen** (Top‑FP/FN) stichprobenartig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965afde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def show_feature_importance(model_name: str, model, feature_names):\n",
    "    try:\n",
    "        if hasattr(model, \"named_steps\"):\n",
    "            final = model.named_steps.get(\"clf\", model)\n",
    "        else:\n",
    "            final = model\n",
    "        if hasattr(final, \"coef_\"):\n",
    "            coef = np.ravel(final.coef_)\n",
    "            imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": coef})\n",
    "            imp_df = imp_df.sort_values(\"importance\", key=lambda s: s.abs(), ascending=False)\n",
    "            print(f\"\\nLR-Koeffizienten ({model_name}):\")\n",
    "            display(imp_df)\n",
    "        elif hasattr(final, \"feature_importances_\"):\n",
    "            fi = final.feature_importances_\n",
    "            imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": fi})\n",
    "            imp_df = imp_df.sort_values(\"importance\", ascending=False)\n",
    "            print(f\"\\nRF-Feature Importances ({model_name}):\")\n",
    "            display(imp_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Konnte Feature-Bedeutungen für {model_name} nicht darstellen: {e}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    show_feature_importance(name, model, feature_cols)\n",
    "\n",
    "def error_samples(model, X_te, y_te, df_pairs_test: pd.DataFrame, n=10):\n",
    "    y_pred = model.predict(X_te)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_te)[:,1]\n",
    "    except Exception:\n",
    "        y_proba = np.full_like(y_pred, fill_value=np.nan, dtype=float)\n",
    "    df = df_pairs_test.copy()\n",
    "    df[\"y_true\"] = y_te\n",
    "    df[\"y_pred\"] = y_pred\n",
    "    df[\"score\"] = y_proba\n",
    "    fp = df[(df.y_true==0) & (df.y_pred==1)].sort_values(\"score\", ascending=False).head(n)\n",
    "    fn = df[(df.y_true==1) & (df.y_pred==0)].sort_values(\"score\", ascending=True).head(n)\n",
    "    return fp, fn\n",
    "\n",
    "pairs_test = pairs.iloc[test_idx].copy()\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nFehleranalyse – {name}\")\n",
    "    fp, fn = error_samples(model, X_test, y_test, pairs_test, n=10)\n",
    "    print(\"Top False Positives:\")\n",
    "    display(fp[[\"u\",\"v\",\"label\",\"score\",\"cos_sim\",\"time_gap_s\",\"same_speaker\",\"turn_distance\"]])\n",
    "    print(\"Top False Negatives:\")\n",
    "    display(fn[[\"u\",\"v\",\"label\",\"score\",\"cos_sim\",\"time_gap_s\",\"same_speaker\",\"turn_distance\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6ab72",
   "metadata": {},
   "source": [
    "## 7) Interpretation & Integration in die Pipeline\n",
    "\n",
    "**Interpretation:** Ein ML‑basiertes Paar‑Modell kann **schwierige Fälle** besser unterscheiden, da es mehrere Signale kombiniert (Semantik + Zeit + Struktur).  \n",
    "In ersten Experimenten sind insbesondere `cos_sim` und `time_gap_s` oft starke Prädiktoren; `overlap` und `same_speaker` liefern zusätzlichen Kontext.\n",
    "\n",
    "**Integration in Task‑5D Pipeline (Vorschlag):**\n",
    "1. Erzeuge für jede Session einen **Graphen**: Knoten = Utterances, Kante `(i, j)` wenn `P(same_conversation) ≥ τ` und ggf. `|Δt| ≤ W` (Heuristik).  \n",
    "2. Bestimme **verbundene Komponenten** oder wende **community detection** / **graph clustering** an, um Gesprächsgruppen zu erhalten.  \n",
    "3. **Kalibriere** den Schwellwert `τ` per Dev‑Set auf **conversation‑level** Metriken (z. B. ARI, B‑Cubed F1), nicht nur pair‑level F1.  \n",
    "4. Kombiniere mit Distanz‑ oder Overlap‑Heuristiken aus Teil 4 für **Hybrid‑Ansätze** (z. B. nur Kandidaten‑Kanten innerhalb eines Zeitfensters).\n",
    "\n",
    "**Bewertung:** Ob der Ansatz die Gesprächstrennung **robuster** macht, zeigt sich daran, ob die resultierenden Konversationscluster auf Session‑Ebene stabiler und näher an den Referenzlabels liegen als beim rein distanzbasierten Verfahren. Für eine faire Bewertung sollte dieselbe Train/Test‑Aufteilung je Session verwendet werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e084aeb",
   "metadata": {},
   "source": [
    "## 8) Persistenz & Nächste Schritte\n",
    "\n",
    "- Das Paar‑Dataset wurde unter `data/derived/part5_pairs/pairs_dataset.parquet` abgelegt.  \n",
    "- Modelle lassen sich optional mit `joblib` speichern und später wiederverwenden.  \n",
    "- **Nächste Schritte:**\n",
    "  - Hyperparameter‑Suche (z. B. `class_weight`, `C`, `max_depth`, `threshold τ`)\n",
    "  - Threshold‑Kalibrierung gegen konversationsbasierte Metriken (ARI, B‑Cubed)\n",
    "  - Erweiterte Merkmale (z. B. Sprecherwechsel‑Dichte, semantische Themenwechsel, Prosodie‑Features)\n",
    "  - Graph‑basierte Post‑Processing‑Strategien\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b48cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Modelle speichern (bei Bedarf aktivieren)\n",
    "# from joblib import dump\n",
    "# from pathlib import Path\n",
    "# def ensure_dir(p: Path):\n",
    "#     p.mkdir(parents=True, exist_ok=True)\n",
    "# ensure_dir(Path(\"data/derived/part5_pairs/models\"))\n",
    "# for name, model in models.items():\n",
    "#     outp = Path(\"data/derived/part5_pairs/models\") / f\"{name}_pair_classifier.joblib\"\n",
    "#     dump(model, outp)\n",
    "#     print(\"Gespeichert:\", outp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
