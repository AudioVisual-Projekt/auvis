{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60c8f0c",
   "metadata": {},
   "source": [
    "# Task 5D – Teil 1: Setup & Daten-Download\n",
    "\n",
    "**Umfang dieses Notebooks (Teil 1):**\n",
    "\n",
    "1. Überprüfung der Umgebung (JupyterHub, Linux, optional GPU)\n",
    "2. Anlegen einer sauberen Projektverzeichnisstruktur\n",
    "3. (Optional) Installation notwendiger Python-Pakete im lokalen User-Scope\n",
    "4. Anmeldung bei Hugging Face (HF-Token)\n",
    "5. Download der benötigten Archive (z. B. `train_without_central_videos.zip`, `dev_without_central_videos.zip`)\n",
    "6. Entpacken der Archive nach `data/raw/{train,dev}`\n",
    "7. Sanity-Checks ausführen (Erwartung: `speaker_to_cluster.json` und `spk_*.vtt` vorhanden)\n",
    "\n",
    "> **Hinweis:** Dies ist ein minimaler, reproduzierbarer Einstieg. Es werden **keine Änderungen am Projekt-Repository** vorgenommen. Das Notebook ist dafür gedacht, direkt in deiner **JupyterHub-Umgebung** ausgeführt zu werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66d737",
   "metadata": {},
   "source": [
    "\n",
    "## Reproduzierbarkeit & Konventionen\n",
    "\n",
    "* Alle Arbeitsschritte erfolgen innerhalb des **JupyterHub-Home-Verzeichnisses**.\n",
    "* Es wird ein eigenes Projekt-Root verwendet: `~/task5D_ml_prototype/`\n",
    "* Datenstruktur:\n",
    "\n",
    "  ```\n",
    "  task5D_ml_prototype/\n",
    "  ├── data/\n",
    "  │   ├── raw/\n",
    "  │   │   ├── train/      # entpacktes Trainings-Set\n",
    "  │   │   └── dev/        # entpacktes Entwicklungs-Set\n",
    "  │   └── downloads/      # ursprüngliche .zip-Dateien\n",
    "  └── run_config.json     # speichert Pfade und HF-Repo-Metadaten für Reproduzierbarkeit\n",
    "  ```\n",
    "* Bei Bedarf kann der Speicherort angepasst werden, indem die Variable `PROJECT_ROOT` entsprechend geändert wird.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca03627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]\n",
      "Platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35\n",
      "Hostname: jupyter-ercel001\n",
      "Time: 2025-09-13 09:25:21.611124\n",
      "PyTorch: 2.4.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-80GB\n",
      "OK: /home/ercel001/AUVIS/task5D_ml_prototype\n",
      "OK: /home/ercel001/AUVIS/task5D_ml_prototype/data\n",
      "OK: /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads\n",
      "OK: /home/ercel001/AUVIS/task5D_ml_prototype/data/raw\n",
      "OK: /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train\n",
      "OK: /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev\n",
      "\n",
      "Gespeichert: /home/ercel001/AUVIS/task5D_ml_prototype/run_config.json\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# 1) Umgebungs-Check & Verzeichnis-Bootstrap\n",
    "# ------------------------------------------\n",
    "# Zweck dieses Blocks:\n",
    "# - Ausgabe zentraler Systeminformationen (Python-Version, OS, Hostname, Uhrzeit) zur schnellen Einordnung von Logs\n",
    "# - Optionaler Test, ob PyTorch/GPU verfügbar ist (rein informativ; für Download/Entpacken nicht erforderlich)\n",
    "# - Anlage einer sauberen Projektverzeichnisstruktur unter ~/AUVIS/task5D_ml_prototype\n",
    "# - Persistenz einer minimalen Konfigurationsdatei (run_config.json) für Reproduzierbarkeit der Pfade\n",
    "\n",
    "import sys, platform, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Systeminformationen (für spätere Fehlersuche hilfreich) ---\n",
    "print(\"Python:\", sys.version)               # Beispiel: \"3.11.6 (main, ...)\"\n",
    "print(\"Platform:\", platform.platform())     # Beispiel: \"Linux-5.15.0-...-x86_64-with-glibc2.35\"\n",
    "print(\"Hostname:\", platform.node())         # Knoten-/Rechnername im Cluster\n",
    "print(\"Time:\", datetime.now())              # Zeitstempel für Nachvollziehbarkeit\n",
    "\n",
    "# --- Optionaler GPU-Check via PyTorch ---\n",
    "# Hinweis: Für Teil 1 (Download/Entpacken) nicht notwendig. Die Ausgabe dient nur als Statusinformation.\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    # PyTorch ist ggf. nicht installiert oder nicht initialisierbar; für diesen Schritt unkritisch.\n",
    "    print(\"PyTorch nicht verfügbar (für diesen Schritt unkritisch).\", e)\n",
    "\n",
    "# --- Projektverzeichnis festlegen ---\n",
    "# PROJECT_ROOT definiert den Arbeitsordner für das Task-5D-Teilprojekt.\n",
    "# Anpassung möglich, z. B. auf ein schnelleres Storage (Scratch).\n",
    "PROJECT_ROOT = Path.home() / \"AUVIS/task5D_ml_prototype\"\n",
    "\n",
    "# --- Unterordner für klar definierte Datenablage ---\n",
    "DATA_DIR  = PROJECT_ROOT / \"data\"       # Oberordner für alle Daten\n",
    "DL_DIR    = DATA_DIR / \"downloads\"      # Ablage für ursprüngliche ZIP-Downloads\n",
    "RAW_DIR   = DATA_DIR / \"raw\"            # Zielbasis für entpackte Inhalte\n",
    "TRAIN_DIR = RAW_DIR / \"train\"           # Entpacktes Trainings-Set\n",
    "DEV_DIR   = RAW_DIR / \"dev\"             # Entpacktes Entwicklungs-Set\n",
    "\n",
    "# --- Verzeichnisse anlegen (idempotent) ---\n",
    "# parents=True erzeugt ggf. fehlende Zwischenordner.\n",
    "# exist_ok=True sorgt dafür, dass kein Fehler geworfen wird, falls Ordner bereits existieren.\n",
    "for p in [PROJECT_ROOT, DATA_DIR, DL_DIR, RAW_DIR, TRAIN_DIR, DEV_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"OK:\", p)\n",
    "\n",
    "# --- Minimale Lauf-Konfiguration schreiben ---\n",
    "# Die Datei run_config.json dient als \"Anker\" für Pfade/Metadaten und erleichtert reproduzierbare Ausführung.\n",
    "RUN_CONFIG = {\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"data\": {\n",
    "        \"downloads\": str(DL_DIR),\n",
    "        \"raw_train\": str(TRAIN_DIR),\n",
    "        \"raw_dev\": str(DEV_DIR),\n",
    "    },\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"notes\": \"Task 5D – Part 1 setup created.\"\n",
    "}\n",
    "\n",
    "with open(PROJECT_ROOT / \"run_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(RUN_CONFIG, f, indent=2)\n",
    "\n",
    "print(\"\\nGespeichert:\", PROJECT_ROOT / \"run_config.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb3493",
   "metadata": {},
   "source": [
    "## 2) (Optional) Abhängigkeiten\n",
    "\n",
    "Für **Teil 1** (Download und Entpacken) werden lediglich die Standardbibliotheken sowie das Paket `huggingface_hub` benötigt.\n",
    "\n",
    "Falls das Paket noch nicht vorhanden ist, erfolgt die Installation im **User-Scope**; Administratorrechte sind dafür nicht erforderlich.\n",
    "\n",
    "> Für GPU-gestützte Embeddings wird in **Teil 3** optional das Paket `sentence-transformers` ergänzt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65aa4e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub bereits verfügbar: 0.29.3\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# 2) (Optional) Abhängigkeiten installieren/prüfen\n",
    "# ------------------------------------------------\n",
    "# Zweck dieses Blocks:\n",
    "# - Prüfen, ob das Paket \"huggingface_hub\" verfügbar ist\n",
    "# - Falls nicht vorhanden, Installation im User-Scope (keine Administratorrechte erforderlich)\n",
    "# Hintergrund:\n",
    "# - \"huggingface_hub\" stellt hf_hub_download & Login bereit, um Dateien aus HF-Dataset-Repositories zu laden.\n",
    "\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    print(\"huggingface_hub bereits verfügbar:\", huggingface_hub.__version__)\n",
    "except ImportError:\n",
    "    print(\"huggingface_hub nicht gefunden. Installation wird durchgeführt ...\")\n",
    "    # %pip ist in Jupyter der empfohlene Weg, Pakete in die laufende Kernel-Umgebung zu installieren.\n",
    "    # --user: Installation in das Benutzerverzeichnis (keine Admin-Rechte)\n",
    "    # -q: weniger Ausgabemenge; für ausführlichere Logs kann -q entfernt werden.\n",
    "    %pip install -q --user \"huggingface_hub>=0.24,<0.26\"\n",
    "    import huggingface_hub\n",
    "    print(\"Installation abgeschlossen. Version:\", huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571ea36",
   "metadata": {},
   "source": [
    "## 3) Anmeldung bei Hugging Face\n",
    "\n",
    "Für den Zugriff wird ein gültiges **HF Access Token** mit **Lese-Berechtigung** benötigt. Zwei Varianten stehen zur Verfügung:\n",
    "\n",
    "* **Umgebungsvariable (empfohlen):** Das Token wird in der Jupyter-Umgebung als `HUGGINGFACE_TOKEN` gesetzt.\n",
    "* **Interaktive Eingabe:** Das Token wird bei der Abfrage im Notebook manuell eingefügt (Eingabe bleibt verborgen).\n",
    "\n",
    "Die Zugangsdaten werden anschließend über `huggingface_hub.login(...)` im **Credential Store** hinterlegt und stehen damit auch in späteren Notebook-Zellen zur Verfügung.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fda8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Angemeldet als: HumanError91\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3) Anmeldung bei Hugging Face\n",
    "# -----------------------------\n",
    "# Zweck dieses Blocks:\n",
    "# - Laden eines persönlichen Hugging-Face-Tokens aus einer Datei im Projektverzeichnis\n",
    "# - Anmeldung mit Lese-Berechtigung (read), um Datensätze herunterladen zu können\n",
    "# Sicherheitsaspekt:\n",
    "# - Das Token liegt außerhalb des Notebooks (keine Klartext-Hinterlegung im Code)\n",
    "# - Rechte der Datei sollten restriktiv gesetzt sein (z. B. chmod 600)\n",
    "\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "token_file = PROJECT_ROOT / \".huggingface_token\"  # erwarteter Speicherort des Tokens (projektspezifisch)\n",
    "\n",
    "# Datei lesen und Token validieren\n",
    "with open(token_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "assert token, \"Hugging-Face-Token-Datei fehlt oder ist leer.\"\n",
    "\n",
    "# Anmeldung: nur Lese-Rechte erforderlich; Credentials werden für Folgezellen hinterlegt\n",
    "login(token=token, write_permission=False, add_to_git_credential=True)\n",
    "\n",
    "# Kurze Rückmeldung zur Kontrolle\n",
    "print(\"Angemeldet als:\", whoami().get(\"name\", \"<unbekannt>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5885db",
   "metadata": {},
   "source": [
    "## 4) Datenquelle (Hugging Face)\n",
    "\n",
    "Es wird ein **Dataset-Repository-Identifier** sowie eine Liste der erwarteten **Dateinamen** definiert, die heruntergeladen werden sollen.\n",
    "Befinden sich die Dateien in einem Hugging-Face-*Dataset-Repository*, bleibt `repo_type=\"dataset\"` unverändert.\n",
    "\n",
    "> `REPO_ID` ist durch den projektspezifischen Repository-Namen zu ersetzen (z. B. `\"my-org/mcorec-task1-derivatives\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b91981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MCoRecChallenge/MCoRec'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['train_without_central_videos.zip', 'dev_without_central_videos.zip']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 4) Datenquelle definieren (Hugging Face Dataset Repository)\n",
    "# --------------------------------------------------------------\n",
    "# Dieser Abschnitt legt fest:\n",
    "# - Welches Hugging-Face-Dataset-Repository verwendet werden soll (REPO_ID)\n",
    "# - Welche Dateien daraus heruntergeladen werden sollen (FILENAMES)\n",
    "# - Welcher Repository-Typ es ist (REPO_TYP), typischerweise \"dataset\"\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# WICHTIG: Hier den tatsächlichen Repository-Namen auf Hugging Face eintragen.\n",
    "# Beispiel: \"my-org/mcorec-task1-derivatives\"\n",
    "REPO_ID  = \"MCoRecChallenge/MCoRec\"\n",
    "REPO_TYP = \"dataset\"                   # Für Datensätze immer \"dataset\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# Erwartete Dateinamen im Repository – müssen mit dem HF-Dataset-Repo übereinstimmen\n",
    "FILENAMES = [\n",
    "    \"train_without_central_videos.zip\",  # Trainingsdaten (ohne zentrale Kameraspur)\n",
    "    \"dev_without_central_videos.zip\",    # Entwicklungsdaten (ohne zentrale Kameraspur)\n",
    "]\n",
    "\n",
    "# Ausgabe der gesetzten Variablen zur Kontrolle im Notebook\n",
    "display(REPO_ID, REPO_TYP, FILENAMES)\n",
    "\n",
    "# Sicherheitsabfrage:\n",
    "# Bricht die Ausführung ab, wenn REPO_ID nicht angepasst wurde\n",
    "assert \"REPLACE\" not in REPO_ID, \"Bitte REPO_ID auf den tatsächlichen Hugging-Face-Dataset-Namen setzen.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6ec2b",
   "metadata": {},
   "source": [
    "## 5) Archive herunterladen\n",
    "\n",
    "Die Funktion `huggingface_hub.hf_hub_download` wird verwendet, um die angegebenen Dateien in das Verzeichnis `data/downloads/` herunterzuladen.\n",
    "\n",
    "Alternativ können die Dateien auch manuell (z. B. über die Benutzeroberfläche von Hugging Face) heruntergeladen und im Verzeichnis `data/downloads/` abgelegt werden.\n",
    "In diesem Fall sollte anschließend direkt die Zelle zur **Entpackung (Extraction)** ausgeführt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ae3491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herunterladen: train_without_central_videos.zip → /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads/train_without_central_videos.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2158a8afe9469384c0a38125160825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_without_central_videos.zip:   0%|          | 0.00/36.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/68591d18f6c5019c67fb9a44/55d229c2d303442c3f9aeaf09358590a9c392a49329ece4feba231b11c2360ea?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250913T093414Z&X-Amz-Expires=3600&X-Amz-Signature=2c8b817998a6c6dc20e18f462f2fbbcb3a321039d2e9f4f53f585f51ad3113ca&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=6884ce688dce0eec8a19a37f&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train_without_central_videos.zip%3B+filename%3D%22train_without_central_videos.zip%22%3B&response-content-type=application%2Fzip&x-id=GetObject&Expires=1757759654&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Nzc1OTY1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODU5MWQxOGY2YzUwMTljNjdmYjlhNDQvNTVkMjI5YzJkMzAzNDQyYzNmOWFlYWYwOTM1ODU5MGE5YzM5MmE0OTMyOWVjZTRmZWJhMjMxYjExYzIzNjBlYSoifV19&Signature=JyUY751oXC9Ov1HIiWGtEd3fOw4CjSxduL2kQbHbePeuRkZTjh1D95Z5WZchowwNZzoCOESl6glFk81LTxeCFacAtuKNte48b8DTK%7EJzFO6P6vDYKHqXHNSqQ0MzYYo7iyjzCZu1jGW57gqx0WrpR24pFLpSxSARbF0Cf9jcSEc6jqkrPM6QM31OTCElEqwR05VChUEGbfcDklSh6zrwBqhHYFA7MPxLgCi6iFuVoYcjLDpCHhuEjJ2EFgt8ttwENHjQvJ8Hvij7ciLVeYCx5qK2yyg1cWZIyQRdD1MYIkh0Kh9A3YxnBTG0qT1CkD%7E0hVXKDEmvubCgr3vREs7KEA__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4a35f1f949460a9dab6764b6d222a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_without_central_videos.zip:  26%|##6       | 9.50G/36.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads/train_without_central_videos.zip\n",
      "Herunterladen: dev_without_central_videos.zip → /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads/dev_without_central_videos.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c007ceb1b3f54aa88f928317480d0782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev_without_central_videos.zip:   0%|          | 0.00/6.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads/dev_without_central_videos.zip\n",
      "\n",
      "Heruntergeladene Dateien:\n",
      " - /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads/train_without_central_videos.zip (36,233,911,826 Bytes)\n",
      " - /home/ercel001/AUVIS/task5D_ml_prototype/data/downloads/dev_without_central_videos.zip (6,124,183,644 Bytes)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 5) Archive herunterladen\n",
    "# ------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Die in FILENAMES definierten ZIP-Dateien vom Hugging-Face-Repository herunterladen\n",
    "# - Zielordner: DL_DIR (data/downloads/)\n",
    "# - Existierende Dateien mit gültiger Größe werden übersprungen\n",
    "# - Fortschritt und Dateigröße werden ausgegeben\n",
    "\"\"\"\n",
    "import shutil                     # Für Dateikopien (copy2 = inkl. Metadaten)\n",
    "from huggingface_hub import hf_hub_download  # Zum Herunterladen aus HF-Datasets\n",
    "from tqdm import tqdm             # Optional: Fortschrittsanzeige (hier nicht genutzt, aber vorbereitet)\n",
    "\n",
    "targets = []  # Liste aller lokal verarbeiteten Zieldateien\n",
    "\n",
    "for fname in FILENAMES:\n",
    "    dst = DL_DIR / fname  # Zielpfad der Datei im Downloadverzeichnis\n",
    "    \n",
    "    # Wenn die Datei bereits vorhanden ist und eine sinnvolle Größe hat → überspringen\n",
    "    if dst.exists() and dst.stat().st_size > 0:\n",
    "        print(f\"Übersprungen (bereits vorhanden): {dst}\")\n",
    "        targets.append(dst)\n",
    "        continue\n",
    "\n",
    "    # Andernfalls: Herunterladen und in DL_DIR kopieren\n",
    "    print(f\"Herunterladen: {fname} → {dst}\")\n",
    "    \n",
    "    # Lokaler temporärer Speicherort aus dem HF-Cache\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=REPO_ID,         # ID des Datasets auf Hugging Face\n",
    "        filename=fname,          # Name der Datei, die geladen werden soll\n",
    "        repo_type=REPO_TYP       # Typ des Repositories (z. B. \"dataset\")\n",
    "    )\n",
    "    \n",
    "    # Kopieren vom Cache-Verzeichnis in das eigene Projektverzeichnis\n",
    "    shutil.copy2(local_path, dst)\n",
    "    print(\"Gespeichert:\", dst)\n",
    "    \n",
    "    targets.append(dst)\n",
    "\n",
    "# Zusammenfassung: Alle verarbeiteten Zieldateien mit Größenangabe\n",
    "print(\"\\nHeruntergeladene Dateien:\")\n",
    "for t in targets:\n",
    "    print(\" -\", t, f\"({t.stat().st_size:,} Bytes)\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91092658",
   "metadata": {},
   "source": [
    "## 6) Archive entpacken\n",
    "\n",
    "Die Archivdateien mit dem Präfix `train_*.zip` werden nach `data/raw/train/` entpackt,\n",
    "die Dateien mit dem Präfix `dev_*.zip` nach `data/raw/dev/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690b71a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entpacke: train_without_central_videos.zip → /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train\n",
      "Fertig.\n",
      "Entpacke: dev_without_central_videos.zip → /home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev\n",
      "Fertig.\n",
      "\n",
      "Dateiübersicht nach Entpacken:\n",
      "train: 5806 Dateien\n",
      "dev:   1553 Dateien\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# 6) Archive entpacken\n",
    "# ---------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Die heruntergeladenen ZIP-Dateien aus der Liste `targets` entpacken\n",
    "# - Dateien mit \"train\" im Namen → nach TRAIN_DIR (data/raw/train/)\n",
    "# - Dateien mit \"dev\" im Namen → nach DEV_DIR (data/raw/dev/)\n",
    "# - Sonstige ZIPs (falls vorhanden) → nach RAW_DIR (data/raw/)\n",
    "# - Danach: Ausgabe, wie viele Dateien in train/dev enthalten sind\n",
    "\"\"\"\n",
    "import zipfile  # Für das Arbeiten mit ZIP-Archiven\n",
    "\n",
    "def extract_zip(zip_path: Path, out_dir: Path):\n",
    "    \"\"\"\n",
    "    # Entpackt eine ZIP-Datei in das angegebene Zielverzeichnis.\n",
    "    # Das Zielverzeichnis wird bei Bedarf angelegt.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Entpacke: {zip_path.name} → {out_dir}\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(out_dir)\n",
    "    print(\"Fertig.\")\n",
    "\n",
    "# Hauptschleife: Alle ZIP-Dateien aus der Download-Liste verarbeiten\n",
    "for zf in targets:\n",
    "    name = zf.name.lower()\n",
    "    if \"train\" in name:\n",
    "        extract_zip(zf, TRAIN_DIR)\n",
    "    elif \"dev\" in name:\n",
    "        extract_zip(zf, DEV_DIR)\n",
    "    else:\n",
    "        print(\"Kein Split erkannt für:\", zf.name, \"→ entpacke nach raw/\")\n",
    "        extract_zip(zf, RAW_DIR)\n",
    "\n",
    "# Kontrollausgabe: Wie viele Dateien liegen nun in train/dev\n",
    "print(\"\\nDateiübersicht nach Entpacken:\")\n",
    "print(\"train:\", len(list(TRAIN_DIR.rglob('*'))), \"Dateien\")\n",
    "print(\"dev:  \", len(list(DEV_DIR.rglob('*'))), \"Dateien\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b2fb5",
   "metadata": {},
   "source": [
    "## 7) Plausibilitätsprüfung (Sanity Checks)\n",
    "\n",
    "Für jede Session werden mindestens folgende Dateien erwartet:\n",
    "\n",
    "* `speaker_to_cluster.json`\n",
    "* eine oder mehrere Dateien vom Typ `spk_*.vtt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2f6da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Cluster-Dateien: 56 | VTT-Dateien: 291\n",
      "Dev:   Cluster-Dateien: 25 | VTT-Dateien: 139\n",
      "train cluster: data/raw/train/train/session_71/labels/speaker_to_cluster.json\n",
      "train vtt: data/raw/train/train/session_71/labels/spk_0.vtt\n",
      "dev cluster: data/raw/dev/dev/session_51/labels/speaker_to_cluster.json\n",
      "dev vtt: data/raw/dev/dev/session_51/labels/spk_5.vtt\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 7) Plausibilitätsprüfung (Sanity Checks)\n",
    "# ----------------------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Überprüfung, ob alle erwarteten Kern-Dateien pro Session vorhanden sind:\n",
    "#     - speaker_to_cluster.json\n",
    "#     - spk_*.vtt (zeitgestempelte Transkripte)\n",
    "# - Durchführung der Prüfung separat für Trainings- und Entwicklungsdaten\n",
    "# - Frühes Erkennen von Pfadfehlern oder unvollständigem Entpacken\n",
    "\n",
    "def scan_split(root: Path):\n",
    "    \"\"\"\n",
    "    Durchsucht rekursiv ein Verzeichnis nach:\n",
    "    - speaker_to_cluster.json (Cluster-Zuordnung)\n",
    "    - spk_*.vtt (Einzelaussagen je Sprecher)\n",
    "    Gibt jeweils eine Liste zurück.\n",
    "    \"\"\"\n",
    "    clusters = list(root.rglob(\"speaker_to_cluster.json\"))\n",
    "    vtts     = list(root.rglob(\"spk_*.vtt\"))\n",
    "    return clusters, vtts\n",
    "\n",
    "# Suche in train/ und dev/\n",
    "train_clusters, train_vtts = scan_split(TRAIN_DIR)\n",
    "dev_clusters, dev_vtts     = scan_split(DEV_DIR)\n",
    "\n",
    "# Ausgabe der Gesamtzahlen\n",
    "print(\"Train: Cluster-Dateien:\", len(train_clusters), \"| VTT-Dateien:\", len(train_vtts))\n",
    "print(\"Dev:   Cluster-Dateien:\", len(dev_clusters),   \"| VTT-Dateien:\", len(dev_vtts))\n",
    "\n",
    "# Frühzeitige Prüfungen zur Fehlervermeidung bei Folgeoperationen\n",
    "assert len(train_clusters) > 0, \"Keine speaker_to_cluster.json in raw/train gefunden – Entpackung prüfen.\"\n",
    "assert len(train_vtts) > 0, \"Keine spk_*.vtt in raw/train gefunden – Entpackung prüfen.\"\n",
    "assert len(dev_clusters) > 0, \"Keine speaker_to_cluster.json in raw/dev gefunden – Entpackung prüfen.\"\n",
    "assert len(dev_vtts) > 0, \"Keine spk_*.vtt in raw/dev gefunden – Entpackung prüfen.\"\n",
    "\n",
    "# Beispielhafte Ausgabe einer Cluster- und VTT-Datei pro Split (zur Sichtkontrolle)\n",
    "sample_files = [\n",
    "    (\"train cluster\", train_clusters[0] if train_clusters else None),\n",
    "    (\"train vtt\",     train_vtts[0] if train_vtts else None),\n",
    "    (\"dev cluster\",   dev_clusters[0] if dev_clusters else None),\n",
    "    (\"dev vtt\",       dev_vtts[0] if dev_vtts else None),\n",
    "]\n",
    "for label, path in sample_files:\n",
    "    if path:\n",
    "        print(f\"{label}: {path.relative_to(PROJECT_ROOT)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6290db66",
   "metadata": {},
   "source": [
    "## 8) Minimale Konfiguration speichern\n",
    "\n",
    "Speichert `REPO_ID`, Dateinamen sowie die aufgelösten Pfade in `run_config.json`,\n",
    "um die Nachvollziehbarkeit (Traceability) der Datenquelle sicherzustellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613897ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktualisiert: /home/ercel001/AUVIS/task5D_ml_prototype/run_config.json\n",
      "{\n",
      "  \"project_root\": \"/home/ercel001/AUVIS/task5D_ml_prototype\",\n",
      "  \"data\": {\n",
      "    \"downloads\": \"/home/ercel001/AUVIS/task5D_ml_prototype/data/downloads\",\n",
      "    \"raw_train\": \"/home/ercel001/AUVIS/task5D_ml_prototype/data/raw/train\",\n",
      "    \"raw_dev\": \"/home/ercel001/AUVIS/task5D_ml_prototype/data/raw/dev\"\n",
      "  },\n",
      "  \"created_at\": \"2025-09-13T09:25:40\",\n",
      "  \"notes\": \"Task 5D \\u2013 Part 1 setup created.\",\n",
      "  \"hf\": {\n",
      "    \"repo_id\": \"MCoRecChallenge/MCoRec\",\n",
      "    \"repo_type\": \"dataset\",\n",
      "    \"filenames\": [\n",
      "      \"train_without_central_videos.zip\",\n",
      "      \"dev_without_central_videos.zip\"\n",
      "    ]\n",
      "  }\n",
      "} ...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# 8) Minimale Konfiguration speichern\n",
    "# ------------------------------------\n",
    "# Ziel dieses Blocks:\n",
    "# - Ergänzen der bestehenden Datei `run_config.json` um:\n",
    "#     - Hugging Face Repository-ID (REPO_ID)\n",
    "#     - Repository-Typ (z. B. \"dataset\")\n",
    "#     - Dateinamen der heruntergeladenen ZIP-Archive\n",
    "# - Dadurch ist die Herkunft der Daten später eindeutig nachvollziehbar (Traceability)\n",
    "# - Die Datei liegt im Projektverzeichnis unter: PROJECT_ROOT/run_config.json\n",
    "\n",
    "RUN_CONFIG_UPDATE = {\n",
    "    \"hf\": {\n",
    "        \"repo_id\": REPO_ID,        # z. B. \"MCoRecChallenge/MCoRec\"\n",
    "        \"repo_type\": REPO_TYP,     # meist \"dataset\"\n",
    "        \"filenames\": FILENAMES,    # Liste der verwendeten ZIP-Dateien\n",
    "    }\n",
    "}\n",
    "\n",
    "# Pfad zur bestehenden Konfigurationsdatei\n",
    "cfg_path = PROJECT_ROOT / \"run_config.json\"\n",
    "\n",
    "# 1) Vorhandene Konfiguration einlesen\n",
    "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# 2) Aktualisierung mit den neuen HF-Infos\n",
    "cfg.update(RUN_CONFIG_UPDATE)\n",
    "\n",
    "# 3) Zurückschreiben der aktualisierten Konfiguration\n",
    "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "# 4) Ausgabe zur Kontrolle (nur die ersten 800 Zeichen)\n",
    "print(\"Aktualisiert:\", cfg_path)\n",
    "print(json.dumps(cfg, indent=2)[:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae00ce",
   "metadata": {},
   "source": [
    "### ✅ Nächste Schritte (für Teil 2)\n",
    "\n",
    "Fahre mit dem nächsten Notebook **Teil 2: Datenladen & Äußerungsextraktion** fort, sobald die Sanity Checks erfolgreich waren:\n",
    "\n",
    "* Parsen der `spk_*.vtt`-Dateien in ein normalisiertes DataFrame mit den Spalten:\n",
    "  `session_id`, `speaker_id`, `utt_id`, `start_s`, `end_s`, `text`\n",
    "* Laden der `speaker_to_cluster.json`-Dateien pro Session\n",
    "* Verknüpfen der Äußerungen mit den zugehörigen Cluster-Zuordnungen\n",
    "* Persistieren als **Parquet-Datei** zur schnellen Wiederverwendung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439f0d8-c263-4d1c-8e0c-5f36543fe954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
